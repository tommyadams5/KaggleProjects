{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tommyadams/creating-s-p-500-sectors-with-pytorch-kmeans?scriptVersionId=141757172\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Creating New S&P 500 Sectors with Kmeans","metadata":{}},{"cell_type":"markdown","source":"\n<center><img src='https://preview.redd.it/q51lnf0wru471.png?width=960&crop=smart&auto=webp&s=9c63accb214de5c01e7a685088484798856516e1' height=800px width=1000px></center>\n\n## Project Summary\n\nThe stock market is a complex system with many different types of companies. One way to make sense of the stock market is to group stocks into sectors. Sectors are groups of stocks that are similar in terms of their industry or economic activity. This can help investors to view the stock market at a macro economic level, determine which stocks are similar to others, and anticipate price moves for a given stock based on changes in its sector.\n\nThe finance industry has traditionally divided the S&P 500 stocks into 11 sectors:\n\n- Energy: Oil and gas companies\n- Materials: Mining and metals companies\n- Industrials: Manufacturing companies\n- Consumer Discretionary: Consumer goods companies that sell non-essential products\n- Consumer Staples: Consumer goods companies that sell essential products\n- Healthcare: Healthcare companies\n- Financials: Banks, insurance companies, and other financial services companies\n- Information Technology: Technology companies\n- Communication Services: Telecom companies and media companies\n- Real Estate: Real estate investment trusts and other real estate companies\n- Utilities: Public utility companies\n\nOne of the main purposes for the sector groupings is to help an investor limit the price volatility of their portfolio by investing in each of the sectors. To serve this purpose, it is important that stocks within a given sector are more correlated with each other than stocks in other sectors. For example, an increase in energy demand should boost the prices of stocks in the energy sector by a similar amount, but should not necessarily affect the prices of stocks outside the energy sector in the same way. \n\nIn this project, I will use a K-means clustering algorithm to group S&P 500 stocks into clusters based on their 5-day percentage price changes, utilizing PyTorch to implement the K-means algorithm. The goal of this project is to determine if I can come up with better sectors than the existing ones for showing which stocks are correlated with each other.","metadata":{}},{"cell_type":"markdown","source":"## Background\n\nK-means clustering is a simple but effective clustering algorithm. It works by first randomly selecting k points in the data set. These points are called the centroids. The algorithm then iteratively assigns each data point to the cluster with the closest centroid. The centroids are then updated to the mean of the points in each cluster. This process is repeated until the centroids no longer move.\n\nPyTorch is a popular deep learning framework that is used for natural language processing, computer vision, and other machine learning tasks. It is also a good choice for clustering tasks because it is fast and efficient.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom functools import partial\ndevice_choice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:33:55.244609Z","iopub.execute_input":"2023-08-15T00:33:55.245011Z","iopub.status.idle":"2023-08-15T00:33:58.273219Z","shell.execute_reply.started":"2023-08-15T00:33:55.244981Z","shell.execute_reply":"2023-08-15T00:33:58.271934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data source is https://www.kaggle.com/datasets/yash16jr/s-and-p-500-all-assets\n# The data lists daily trading information for the stocks in the S&P 500 from 2010 to 2023\n\ndata = pd.read_csv('/kaggle/input/s-and-p-500-all-assets/SnP500 All assets.csv', dtype=str)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:33:58.275202Z","iopub.execute_input":"2023-08-15T00:33:58.27578Z","iopub.status.idle":"2023-08-15T00:34:04.320314Z","shell.execute_reply.started":"2023-08-15T00:33:58.275743Z","shell.execute_reply":"2023-08-15T00:34:04.319214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploring the data set\nprint(data.describe)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:34:04.32386Z","iopub.execute_input":"2023-08-15T00:34:04.324613Z","iopub.status.idle":"2023-08-15T00:34:04.349026Z","shell.execute_reply.started":"2023-08-15T00:34:04.324568Z","shell.execute_reply":"2023-08-15T00:34:04.347949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identifying the column types for each stock ticker\nmatches = data.columns.str.findall(r'(.*)\\.')\nunique_matches = pd.Series([item for row in matches for item in row]).unique()\nprint(unique_matches)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:34:04.352229Z","iopub.execute_input":"2023-08-15T00:34:04.353112Z","iopub.status.idle":"2023-08-15T00:34:04.370271Z","shell.execute_reply.started":"2023-08-15T00:34:04.353067Z","shell.execute_reply":"2023-08-15T00:34:04.369256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Condensing the data frame to only have daily stock price closing information\nclose_columns = [item for row in data.columns.str.findall(r'Close.*') for item in row]\nclosing_prices = data[close_columns]\nclosing_prices.columns = closing_prices.iloc[0,:]\nclosing_prices = closing_prices.drop(index=[0,1]).reset_index(drop=True)\nclosing_prices = closing_prices.astype(float)\nclosing_prices.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:34:04.372135Z","iopub.execute_input":"2023-08-15T00:34:04.372994Z","iopub.status.idle":"2023-08-15T00:34:05.177395Z","shell.execute_reply.started":"2023-08-15T00:34:04.372951Z","shell.execute_reply":"2023-08-15T00:34:05.176488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the rolling 5 day ratio of closing prices. Several stocks did not having pricing data going back to 2010 since they were not part of the S&P 500 then. \n# I decided to drop these stocks and their NA values from this analysis, leaving me with 438 stocks.\n# If higher portion of stocks had NA values, I would have raised the start year from 2010 to remove NA values for these stocks. \ndays_for_price_change = 5\nprice_change = closing_prices.copy()\nprice_change = price_change.dropna(axis=1)\nprice_change1 = price_change.drop(index=list(range(days_for_price_change))).reset_index(drop=True)\nprice_change2 = price_change.drop(index=list(range(len(price_change)-days_for_price_change,len(price_change)))).reset_index(drop=True)\nprice_change = price_change1 / price_change2\nprice_change[:5]","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:34:05.178401Z","iopub.execute_input":"2023-08-15T00:34:05.179066Z","iopub.status.idle":"2023-08-15T00:34:05.247014Z","shell.execute_reply.started":"2023-08-15T00:34:05.179029Z","shell.execute_reply":"2023-08-15T00:34:05.245947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting data frame to a tensor\ndata_array = np.array(price_change.T, dtype=np.float16)\ndata_tensor = torch.from_numpy(data_array)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:34:05.248327Z","iopub.execute_input":"2023-08-15T00:34:05.248721Z","iopub.status.idle":"2023-08-15T00:34:05.272436Z","shell.execute_reply.started":"2023-08-15T00:34:05.248693Z","shell.execute_reply":"2023-08-15T00:34:05.27162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating data frame with mapping of stock tickers to current S&P 500 sectors\nsectors = pd.DataFrame({'Ticker':price_change.columns})\nticker_data = pd.read_csv('/kaggle/input/sp500sectors/constituents.csv', dtype=str)\nsummary = sectors.merge(ticker_data, left_on=['Ticker'], right_on=['Symbol'], how='left')\nsummary = summary.drop(columns=['Symbol'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:34:05.273422Z","iopub.execute_input":"2023-08-15T00:34:05.274281Z","iopub.status.idle":"2023-08-15T00:34:05.299185Z","shell.execute_reply.started":"2023-08-15T00:34:05.274249Z","shell.execute_reply":"2023-08-15T00:34:05.2983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the loss function (aka 'inertia') to measure each grouping of stocks. The lower the inertia, the better the grouping is.\ndef inertia(data, cluster_centers, cluster_matches):\n    centers = torch.stack([cluster_centers[i] for i in cluster_matches])\n    diff = (data - centers)**2\n    diff_total = diff.sum()**0.5\n    return diff_total","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:34:05.300227Z","iopub.execute_input":"2023-08-15T00:34:05.300939Z","iopub.status.idle":"2023-08-15T00:34:05.305641Z","shell.execute_reply.started":"2023-08-15T00:34:05.300901Z","shell.execute_reply":"2023-08-15T00:34:05.304953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the inertia of the existing S&P 500 sectors.\nprice_change_sp_sectors = price_change.T\nprice_change_sp_sectors['Sector']=pd.factorize(summary['Sector'])[0]\ncenters = price_change_sp_sectors.groupby(['Sector']).mean()\nsp_sectors_inertia = inertia(data=data_tensor, cluster_centers=torch.tensor(centers.values), cluster_matches=torch.tensor(price_change_sp_sectors['Sector'].values))\nprint(sp_sectors_inertia)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:34:05.308229Z","iopub.execute_input":"2023-08-15T00:34:05.309225Z","iopub.status.idle":"2023-08-15T00:34:05.493629Z","shell.execute_reply.started":"2023-08-15T00:34:05.309193Z","shell.execute_reply":"2023-08-15T00:34:05.492521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Kmeans algorithm \n\ndef initialize(X, num_clusters, seed, start_clusters):\n\n\n    if seed == None:\n        indices = np.random.choice(len(X), num_clusters, replace=False)\n    else:\n        np.random.seed(seed)\n        indices = np.random.choice(len(X), num_clusters, replace=False)\n        \n    initial_state = X[indices]\n    \n    if len(start_clusters) > 0:\n        for i in range(len(start_clusters)):\n            initial_state[i] = torch.Tensor(start_clusters[i,:])\n    \n    return initial_state\n\ndef kmeans(\n        X,\n        num_clusters,\n        tol=.0001,\n        iter_limit=99,\n        device=torch.device(device_choice),\n        seed=None,\n        start_clusters=[]\n):\n    \n    X = X.float()\n    X = X.to(device)\n    initial_state = initialize(X, num_clusters, seed=seed, start_clusters=start_clusters)\n    iteration = 0\n   \n    while True:\n\n        dis = distance(X, initial_state, device=device)\n        cluster_assign = torch.argmin(dis, dim=1)\n        initial_state_pre = initial_state.clone()\n\n        for index in range(num_clusters):\n            selected = torch.nonzero(cluster_assign == index).squeeze().to(device)\n            selected = torch.index_select(X, 0, selected)\n            \n            # Update cluster centers\n            if selected.shape[0] != 0:\n                initial_state[index] = selected.mean(dim=0)\n\n        cluster_shift = torch.sum(\n            torch.sum((initial_state - initial_state_pre) ** 2, dim=1) ** 0.5\n            )\n\n        # increment iteration\n        iteration = iteration + 1\n\n        if cluster_shift < tol:\n            break\n        if iter_limit != 0 and iteration >= iter_limit:\n            print('iter_limit reached')\n            break\n\n    return cluster_assign.cpu(), initial_state.cpu()\n\ndef distance(dataset, clusters, device=torch.device(device_choice)):\n    \n    # transfer to device\n    dataset, clusters = dataset.to(device), clusters.to(device)\n\n    # Stocks x 1 x Dates\n    A = dataset.unsqueeze(dim=1)\n\n    # 1 x Clusters x Dates\n    B = clusters.unsqueeze(dim=0)\n\n    \n    dis = (A - B) ** 2.0\n    \n    # return Stocks * Clusters matrix for euclidean distance\n    dis = dis.sum(dim=-1) ** 0.5\n    dis = dis.squeeze()\n    return dis\n","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:34:05.494983Z","iopub.execute_input":"2023-08-15T00:34:05.495262Z","iopub.status.idle":"2023-08-15T00:34:05.513589Z","shell.execute_reply.started":"2023-08-15T00:34:05.495237Z","shell.execute_reply":"2023-08-15T00:34:05.509277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating inertia of groupings with different numbers of clusters. \n# The number of clusters is one of the parameters to set with the Kmeans algorithm. Naturally, inertia decreases the more cluster there are,\n# but there may be a critical point at which the marginal benefits of adding new clusters are diminished. Based on the graph, \n# it does not look there is a critical point.\n\nclusters_list = []\ncluster_matches_list = []\ninertias = []\n\nfor cluster_count in range(2,30):\n    cluster_ids_x, cluster_centers = kmeans(\n    X=data_tensor, num_clusters=cluster_count, tol=.0001, seed=np.random.seed(100)\n    )\n    clusters_list.append(cluster_centers)\n    cluster_matches_list.append(cluster_ids_x)\n    \n    inertia_current = inertia(data=data_tensor, cluster_centers=cluster_centers, cluster_matches=cluster_ids_x)\n    inertias.append(inertia_current)\n    \nplt.plot(range(2,30), inertias)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:34:05.514804Z","iopub.execute_input":"2023-08-15T00:34:05.515721Z","iopub.status.idle":"2023-08-15T00:34:33.633021Z","shell.execute_reply.started":"2023-08-15T00:34:05.515686Z","shell.execute_reply":"2023-08-15T00:34:33.631942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a function to run the Kmeans algorithm multiple times. In most situations, including this one, the Kmeans algorithm has multiple relative minimums.\n# To find groupings with low inertia, I used this function to run Kmeans multiple times and then select the grouping with the lowest inertia.\n# For better performance, set the torch device to gpu when calling kmeans_iterate. \n\ndef kmeans_iterate(data, num_clusters, iterations, start_clusters=[], tol=.0001, device=torch.device(device_choice), seed=None): \n\n    clusters_list = []\n    cluster_matches_list = []\n    inertias = []\n\n    for i in range(iterations):\n        if seed != None:\n            seed = seed+1\n        cluster_ids_x, cluster_centers = kmeans(\n        X=data, num_clusters=num_clusters, tol=tol, start_clusters=start_clusters, device=device, seed=seed\n        )\n        clusters_list.append(cluster_centers)\n        cluster_matches_list.append(cluster_ids_x)\n\n        inertia_current = inertia(data=data_tensor, cluster_centers=cluster_centers, cluster_matches=cluster_ids_x)\n        inertias.append(inertia_current)\n\n    \n    \n    return clusters_list, cluster_matches_list, inertias","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:34:33.634674Z","iopub.execute_input":"2023-08-15T00:34:33.635364Z","iopub.status.idle":"2023-08-15T00:34:33.644344Z","shell.execute_reply.started":"2023-08-15T00:34:33.635322Z","shell.execute_reply":"2023-08-15T00:34:33.64321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I ran the Kmeans formula 50 times with 11 clusters and graphed the grouping with the lowest inertia.\n# This inertia is lower than the 37.7590 inertia of the current sectors\nclusters_list, cluster_matches_list, inertias = kmeans_iterate(data_tensor, num_clusters=11, iterations=50, seed=534) \nlowest_inertia_index = inertias.index(min(inertias))\nsummary['Kmeans_Sector'] = cluster_matches_list[lowest_inertia_index] + 1\nsummary.groupby(['Kmeans_Sector', 'Sector'])['Kmeans_Sector'].count().unstack('Sector').plot(kind='bar', stacked=True)\nplt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\nplt.tight_layout()\nplt.show()\nprint(min(inertias))","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:34:33.645543Z","iopub.execute_input":"2023-08-15T00:34:33.645933Z","iopub.status.idle":"2023-08-15T00:35:14.257199Z","shell.execute_reply.started":"2023-08-15T00:34:33.645903Z","shell.execute_reply":"2023-08-15T00:35:14.256154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normally, the starting points for the clusters are randomly selected from the existing data points. In the code below, I tried switching the starting points so that they\n# were closer to the means of the data points, but this did not significantly improve the inertia of the results, so I elected to not set the \n# cluster start points in later iterations. \n\ndata_mean = data_array.mean(axis=0)\nnum_start_clusters = 11\ncluster_locs = np.array([data_mean + np.random.randint(-100,100) * .00001 for i in range(num_start_clusters)])\nclusters_list, cluster_matches_list, inertias = kmeans_iterate(data_tensor, num_clusters=num_start_clusters, iterations=50, start_clusters=cluster_locs) \nlowest_inertia_index = inertias.index(min(inertias))\nsummary['Kmeans_Sector'] = cluster_matches_list[lowest_inertia_index] + 1\nsummary.groupby(['Kmeans_Sector', 'Sector'])['Kmeans_Sector'].count().unstack('Sector').plot(kind='bar', stacked=True)\nplt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\nplt.tight_layout()\nplt.show()\nprint(min(inertias))","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:35:14.258392Z","iopub.execute_input":"2023-08-15T00:35:14.2587Z","iopub.status.idle":"2023-08-15T00:36:05.980976Z","shell.execute_reply.started":"2023-08-15T00:35:14.258674Z","shell.execute_reply":"2023-08-15T00:36:05.979791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Running the Kmeans algorithm 500 times with 7 clusters. I decided to use 7 clusters since it looked like there were multiple small clusters\n# when 11 clusters were used previously. Lowering the cluster count also helps reveal which industries are more similar to each other.\nclusters_list, cluster_matches_list, inertias = kmeans_iterate(data_tensor, num_clusters=7, iterations=500, seed=1000) ","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:36:05.982678Z","iopub.execute_input":"2023-08-15T00:36:05.983326Z","iopub.status.idle":"2023-08-15T00:37:55.426171Z","shell.execute_reply.started":"2023-08-15T00:36:05.983284Z","shell.execute_reply":"2023-08-15T00:37:55.425173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Graphing the 9 groupings with the lowest inertias\nnrows = 3\nncols = 3\npartition_array = np.argsort(inertias)\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(11,11))\nplt.subplots_adjust(hspace=0.3, wspace=0.3)\nfor row in range(nrows):\n    for col in range(ncols):\n        inertia_index = partition_array[3*row+col]\n        summary['Kmeans_Sector'] = cluster_matches_list[inertia_index] + 1\n        summary.groupby(['Kmeans_Sector', 'Sector'])['Kmeans_Sector'].count().unstack('Sector').plot(kind='bar', stacked=True, ax=axes[row, col])\n        axes[row,col].set_xlabel('')\n        axes[row,col].set_title(f'Inertia: {round(float(inertias[inertia_index]),3)}')\n        if row == 0 and col == ncols - 1:\n            axes[row,col].legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', reverse=True)\n        else:\n            axes[row,col].get_legend().remove()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T00:37:55.427586Z","iopub.execute_input":"2023-08-15T00:37:55.428113Z","iopub.status.idle":"2023-08-15T00:37:58.282635Z","shell.execute_reply.started":"2023-08-15T00:37:55.428072Z","shell.execute_reply":"2023-08-15T00:37:58.281466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Graphs\nI showed several stacked bar charts of the groupings that resulted from the Kmeans algorithm. In each bar chart, the y-axis is the number of stocks in each cluster and the x-axis is the cluster number for each cluster. The legend color labels correspond to how the finance industry has currently classified each stock into a sector. \n\n\n## Results\n\nIn the first bar chart, was able to outperform the current sector groupings by generating a lower inertia with 11 clusters than the inertia of the current sector groupings. This means that my clustering algorithm was able to group the stocks more effectively than the traditional sector groupings.\n\nAfter examining the results, I noticed that there were multiple clusters that were quite small. This implied that a smaller number of clusters might be a better fit.\n\nI then ran more iterations with 7 clusters and found a result that still had less inertia than the current sectors despite having 4 less groups. This is a significant improvement, and suggests that the clustering algorithm is able to identify meaningful patterns in the data that the traditional sector groupings do not.\n\nI also looked at the 9 iteration groupings with the lowest inertias. From the bar charts of their groupings, I made the following observations:\n\n- The energy sector is consistently its own group, showing that energy stocks tend to be some of the least correlated stocks to the rest of the market and that this current sector grouping is justified.\n\n- Utilities and real estate industries tend to be grouped together; this makes sense because both sectors have large assets that produce consistent dividends.\n\n- In a majority of the 9 iterations, consumer staples is also in the same group as utilities and real estate. This makes sense because all three of these sectors are somewhat conservative investments in that they produce reliable cash flows but do not have much potential for large gains.\n\n- The industrial and material sectors are often grouped together, probably because the demand for industrial goods also affects the demand for the raw materials that the industrial companies use.\n\n- In half of the 9 iterations, there is a large chunk of financials that is also grouped with the industrial and material sectors. My guess is that this occurs because these are all somewhat cyclical sectors that are tied to the business cycle; when the economy is expanding, there is an increasing demand for goods which leads to expansions in industrials, materials, and financing.\n\n- The rest of the sectors (information technology, health care, consumer discretionary, communication services) tend to be more dispersed across the groupings, implying that these sectors are more diverse and that the stocks in these sectors are not as strongly correlated with each other.\n\n- In particular, consumer discretionary and communication services are consistently dispersed across several groups, implying that these sectors should be removed or reworked to identify a group of stocks that are more correlated with each other.\n\nIn conclusion, I was able to generate stock groupings that better reflected price movement than the existing sectors defined by the finance industry. My recommendation for improving the existing sectors is to rework the consumer discretionary and communication services sectors and possibly the health care and information technology sectors.","metadata":{}}]}