{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tommyadams/fine-tuning-tinyllama?scriptVersionId=142909379\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# <center>Fine Tuning TinyLlama</center>\n    \n    \n<center><img src='https://github.com/jzhang38/TinyLlama/raw/main/.github/TinyLlama_logo.png' height=380px width=380px></center>\n\n## Project Summary\n    \nTinyLlama is a 1.1B Llama model that is currently being trained on 3 trillion tokens, which recently started on September 1st. In this project, I fine-tune the latest version of TinyLlama to generate song lyrics in the style of Taylor Swift. \n\nI used Hugging Face's transformers and peft (parameter-efficient fine-tuning) packages for this project. One of the major challenges of fine-tuning a large language model (LLM) is the high memory usage on the GPU. To address this challenge, I used the quantization and fine-tuning methods described in the 2023 paper \"QLoRA: Efficient Finetuning of Quantized LLMs\". These methods are summarized below:\n\n- Low-rank adaptation: This technique freezes the existing weights of TinyLlama and adds two smaller matrices with lower rank than the weight matrices into the model. Only these two smaller matrices are then trained, instead of all of the model weights. Another way to think of this is that we are grouping weights together and traing a scalar for each group, which is much easier than traing each weight by individually. In addition, low-rank adaptation is only done for the query and values weights in the attention heads of the transformers, while all other areas of the model are frozen. This greatly reduces the computation needed to fine-tune the model, while not impairing performance. \n\n- Double quantization: All weights in TinyLlama are quantized into 4 bits, and the quantization constants are then quantized into 8 bits. This further reduces the memory usage of the model. Low-rank adaptation weights are stored in 16 bits, and model weights are upscaled to 16 bits at computation time. \n\n- NormalFloat data type: The NormalFloat data type is used for quantization. This data type minimizes information loss during quantization by assigning each data point to a quantile bin based on the estimated normal distribution of the data.\n\n- Gradient checkpointing: This technique minimizes the memory storage requirements during training by recalculating some of the gradients from the forward pass instead of storing them all.\n\n- Paged optimizers: This technique enables the CPU to help the GPU with any memory spikes that occur during training, especially when the backward pass reaches a checkpoint. \n\nThese methods collectively enhance the efficiency of the project, enabling the creation of Taylor Swift-style song lyrics while optimizing GPU memory utilization and computational resources.\n\nLink to TinyLlama - https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq\n!pip install bitsandbytes einops wandb -Uqqq","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:30:54.478274Z","iopub.execute_input":"2023-09-13T18:30:54.478962Z","iopub.status.idle":"2023-09-13T18:31:51.481873Z","shell.execute_reply.started":"2023-09-13T18:30:54.478926Z","shell.execute_reply":"2023-09-13T18:31:51.480565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport glob\nimport pandas as pd\nimport numpy as np\nimport re\nfrom peft import get_peft_model, PeftConfig, PeftModel, LoraConfig, prepare_model_for_kbit_training\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, GenerationConfig\nfrom trl import SFTTrainer\nfrom datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:31:51.485224Z","iopub.execute_input":"2023-09-13T18:31:51.485617Z","iopub.status.idle":"2023-09-13T18:32:05.194582Z","shell.execute_reply.started":"2023-09-13T18:31:51.485579Z","shell.execute_reply":"2023-09-13T18:32:05.193554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the dataset \npath = '/kaggle/input/taylor-swift-song-lyrics-all-albums/'\ncsv_files = glob.glob(path + \"/*.csv\")\ndf_list = (pd.read_csv(i) for i in csv_files)\ndf = pd.concat(df_list, ignore_index=True)\nlyrics = '\\n'.join(df.loc[:,'lyric']) \nprint(lyrics[:200])","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:32:05.196217Z","iopub.execute_input":"2023-09-13T18:32:05.197224Z","iopub.status.idle":"2023-09-13T18:32:05.335912Z","shell.execute_reply.started":"2023-09-13T18:32:05.197186Z","shell.execute_reply":"2023-09-13T18:32:05.334977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of all unique characters\nprint(' '.join(sorted(set(lyrics))))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:32:05.338568Z","iopub.execute_input":"2023-09-13T18:32:05.339495Z","iopub.status.idle":"2023-09-13T18:32:05.349315Z","shell.execute_reply.started":"2023-09-13T18:32:05.339459Z","shell.execute_reply":"2023-09-13T18:32:05.347798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning the file by removing/replacing unnecessary characters and removing sections \n# that are not lyrics\nreplace_with_space = ['\\u2005', '\\u200b', '\\u205f', '\\xa0', '-']\nreplace_letters = {'í':'i', 'é':'e', 'ï':'i', 'ó':'o', ';':',', '‘':'\\'', '’':'\\'', ':':',', 'е':'e'} \nremove_list = ['\\)', '\\(', '–','\"','”', '\"', '\\[.*\\]', '.*\\|.*', '—']\n\ncleaned_lyrics = lyrics\n\nfor old, new in replace_letters.items():\n    cleaned_lyrics = cleaned_lyrics.replace(old, new)\nfor string in remove_list:\n    cleaned_lyrics = re.sub(string,'',cleaned_lyrics)\nfor string in replace_with_space:\n    cleaned_lyrics = re.sub(string,' ',cleaned_lyrics)\nprint(''.join(sorted(set(cleaned_lyrics))))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:32:05.350579Z","iopub.execute_input":"2023-09-13T18:32:05.350857Z","iopub.status.idle":"2023-09-13T18:32:05.396171Z","shell.execute_reply.started":"2023-09-13T18:32:05.350822Z","shell.execute_reply":"2023-09-13T18:32:05.395192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting aside a portion for training the model and a portion for testing the data to prevent \n# the model from overfitting to the data it is tested on\nsplit_point = int(len(cleaned_lyrics)*0.95)\ntrain_data = cleaned_lyrics[:split_point]\ntest_data = cleaned_lyrics[split_point:]\ntrain_data_seg = []\nfor i in range(0, len(train_data), 500):\n        text = train_data[i:min(i+500, len(train_data))]\n        train_data_seg.append(text)\ntrain_data_seg = Dataset.from_dict({'text':train_data_seg})\nprint(len(train_data_seg))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:32:05.397597Z","iopub.execute_input":"2023-09-13T18:32:05.397924Z","iopub.status.idle":"2023-09-13T18:32:05.420652Z","shell.execute_reply.started":"2023-09-13T18:32:05.397893Z","shell.execute_reply":"2023-09-13T18:32:05.419761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# You will need to create a Hugging Face account if you do not have one, \n# and then generate a write token to enter in the widget below\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:33:58.450608Z","iopub.execute_input":"2023-09-13T18:33:58.451009Z","iopub.status.idle":"2023-09-13T18:33:58.473811Z","shell.execute_reply.started":"2023-09-13T18:33:58.450976Z","shell.execute_reply":"2023-09-13T18:33:58.472688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the model with double quantization\nmodel_name = \"PY007/TinyLlama-1.1B-step-50K-105b\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,           \n    bnb_4bit_quant_type=\"nf4\",    \n    bnb_4bit_use_double_quant=True, \n    bnb_4bit_compute_dtype=torch.bfloat16, \n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config, \n    device_map=\"auto\",  \n    trust_remote_code=True, \n)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:04.854443Z","iopub.execute_input":"2023-09-13T18:34:04.85483Z","iopub.status.idle":"2023-09-13T18:34:09.453336Z","shell.execute_reply.started":"2023-09-13T18:34:04.854799Z","shell.execute_reply":"2023-09-13T18:34:09.452093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating tokenizer and defining the pad token\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) \ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:09.45539Z","iopub.execute_input":"2023-09-13T18:34:09.456008Z","iopub.status.idle":"2023-09-13T18:34:09.63049Z","shell.execute_reply.started":"2023-09-13T18:34:09.455942Z","shell.execute_reply":"2023-09-13T18:34:09.629261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generating lyrics with the base model. The repetition penalty in the generation config prevents the model from continually repeating the same string.\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndef generate_lyrics(query, model):\n    encoding = tokenizer(query, return_tensors=\"pt\").to(device)\n    generation_config = GenerationConfig(max_new_tokens=250, pad_token_id = tokenizer.eos_token_id,repetition_penalty=1.3, eos_token_id = tokenizer.eos_token_id)\n    outputs = model.generate(input_ids=encoding.input_ids, generation_config=generation_config)\n    text_output = tokenizer.decode(outputs[0],skip_special_tokens=True)\n    print('INPUT\\n', query, '\\n\\nOUTPUT\\n', text_output[len(query):])\ngenerate_lyrics(test_data[200:700], model)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:09.63197Z","iopub.execute_input":"2023-09-13T18:34:09.634524Z","iopub.status.idle":"2023-09-13T18:34:27.510143Z","shell.execute_reply.started":"2023-09-13T18:34:09.634491Z","shell.execute_reply":"2023-09-13T18:34:27.509186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting arguments for low-rank adaptation \n\nmodel = prepare_model_for_kbit_training(model)\n\nlora_alpha = 32 # The weight matrix is scaled by lora_alpha/lora_rank, so I set lora_alpha = lora_rank to remove scaling\nlora_dropout = 0.05 \nlora_rank = 32 \n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_rank,\n    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n    task_type=\"CAUSAL_LM\")\n\npeft_model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:27.515727Z","iopub.execute_input":"2023-09-13T18:34:27.518158Z","iopub.status.idle":"2023-09-13T18:34:28.963773Z","shell.execute_reply.started":"2023-09-13T18:34:27.518121Z","shell.execute_reply":"2023-09-13T18:34:28.962709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting training arguments \n\noutput_dir = \"tommyadams/tinyllama\" # Model repo on your hugging face account where you want to save your model\nper_device_train_batch_size = 3\ngradient_accumulation_steps = 2  \noptim = \"paged_adamw_32bit\" \nsave_strategy=\"steps\" \nsave_steps = 10 \nlogging_steps = 10  \nlearning_rate = 2e-3  \nmax_grad_norm = 0.3 # Sets limit for gradient clipping\nmax_steps = 200     # Number of training steps\nwarmup_ratio = 0.03 # Portion of steps used for learning_rate to warmup from 0\nlr_scheduler_type = \"cosine\" # I chose cosine to avoid learning plateaus\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    lr_scheduler_type=lr_scheduler_type,\n    push_to_hub=True,\n    report_to='none'\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:28.969063Z","iopub.execute_input":"2023-09-13T18:34:28.971384Z","iopub.status.idle":"2023-09-13T18:34:28.982056Z","shell.execute_reply.started":"2023-09-13T18:34:28.971346Z","shell.execute_reply":"2023-09-13T18:34:28.980879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=train_data_seg,\n    peft_config=peft_config,\n    max_seq_length=500,\n    dataset_text_field='text',\n    tokenizer=tokenizer,\n    args=training_arguments\n)\npeft_model.config.use_cache = False","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:28.983634Z","iopub.execute_input":"2023-09-13T18:34:28.984399Z","iopub.status.idle":"2023-09-13T18:34:29.430658Z","shell.execute_reply.started":"2023-09-13T18:34:28.984361Z","shell.execute_reply":"2023-09-13T18:34:29.429642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:29.435093Z","iopub.execute_input":"2023-09-13T18:34:29.435585Z","iopub.status.idle":"2023-09-13T18:40:20.272515Z","shell.execute_reply.started":"2023-09-13T18:34:29.435549Z","shell.execute_reply":"2023-09-13T18:40:20.271435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generating lyrics with fine-tuned model\ngenerate_lyrics(test_data[200:700], model)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:40:20.273656Z","iopub.execute_input":"2023-09-13T18:40:20.274795Z","iopub.status.idle":"2023-09-13T18:41:06.666649Z","shell.execute_reply.started":"2023-09-13T18:40:20.274757Z","shell.execute_reply":"2023-09-13T18:41:06.665543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results\n\nFine-tuning the model for 200 steps on a P100 GPU took about 6 minutes. Before fine-tuning, the model generated a few lines of lyrics in response to the prompt, but then listed some video data from YouTube that it was likely trained on. After fine-tuning, the language model showed improvement in that it learned the common words in Taylor Swift's song lyrics. However, many of the lines were still nonsensical and humorous. To further improve this model, I could start with a larger base model with more parameters (such as Falcon 7b), train the model for longer, and provide longer training segments so that the model can learn song structure in terms of verses and choruses.","metadata":{"execution":{"iopub.status.busy":"2023-09-12T20:25:10.697384Z","iopub.execute_input":"2023-09-12T20:25:10.697755Z","iopub.status.idle":"2023-09-12T20:25:10.703128Z","shell.execute_reply.started":"2023-09-12T20:25:10.697716Z","shell.execute_reply":"2023-09-12T20:25:10.702211Z"}}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}