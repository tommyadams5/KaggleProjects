{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tommyadams/apples-gan?scriptVersionId=141757212\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# <center> Apples GAN </center>\n\n<center><img src='https://www.sagefruit.com/wp-content/uploads/2016/08/our-fruit-apples-1.jpg' height=400px width=400px></center>\n\n## Project Summary\n\nGenerative Adversarial Networks (GANs) have revolutionized the field of machine learning by enabling the creation of synthetic data that closely resembles real-world examples. These networks consist of two key components: the generator and the discriminator, which engage in a competitive learning process. The generator network fabricates images, while the discriminator network learns to distinguish between genuine and synthetic images. Through a tug-of-war dynamic, these networks iteratively refine each other, ultimately leading to the generation of images that are progressively difficult to differentiate from authentic examples. GANs have been used for various applications, and have also sparked concerns due to their association with the creation of manipulated content like deep fakes. \n\nIn this project, I created a GAN using PyTorch and trained it to generate lifelike images of apples. The architecture of the GAN draws inspiration from the 2016 paper titled \"Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks.\" This paper pioneered the usage of convolutional layers in the discriminator network and convolutional-transpose layers in the generator network. Both networks employ a series of convolutional layers, ReLU activation functions, and normalization techniques.\n\nWhile adhering to this general architecture, I made certain modifications to the GAN for this project:\n\n- Sigmoid Activation: Unlike the original paper's use of a tanh function, I used a sigmoid function for the final layer of the generator. This alteration was made to align with the requirements of the image plotting function imshow.\n\n- Enhanced Discriminator Communication: To facilitate enhanced communication between different areas of the image, the discriminator used two linear layers after its convolutions. \n\n- Dropout Regularization: Overfitting is a common concern in deep learning. To mitigate this, a dropout layer was integrated into the discriminator. This regularization technique aids in preventing the discriminator from becoming overly specialized to the training dataset, thus promoting better generalization.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport numpy as np\n\ntorch.manual_seed(300)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset from https://www.kaggle.com/datasets/moltean/fruits\n# Only used the apple images\npath = 'path'\nimg_paths = glob.glob(path + \"/*/*jpg\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Resizing images and converting to RGB array\nimg_list = []\nfor i in img_paths[:]:\n    try:\n        img = cv2.imread(i) \n        img = cv2.resize(img, (64,64)) # Resizing\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Converting to RGB\n        img_list.append(img)\n    except:\n        pass\n\npaints = np.stack(img_list, axis=-1)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting images to a tensor (Img #, RGB #, pixel rows, pixel columns)\npaints = paints.transpose(3, 2, 0, 1)\npaints = torch.from_numpy(paints).int()\nprint(paints.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting one of the training images\nplt.imshow(paints[0].transpose(0, 2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Descriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, 6, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(6),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(6, 12, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(12),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(12, 24, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(24),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(24, 48, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(48),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(48, 96, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(96),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n        self.linear1 = nn.Linear(384, 60)\n        self.linear2 = nn.Linear(60, 1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.view(-1, 384)\n        x = self.linear1(x)\n        x = F.relu(x) + x # \n        x = F.dropout(x, 0.3) # Prevents descriminator from overfitting the real images\n        x = self.linear2(x)\n        x = torch.sigmoid(x)\n        return x\n\n# Testing the output. Descriminator should return a value between 0 and 1 for classifying an image as fake or real\nmodel = Descriminator()\nmodel(paints[:3].float())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = nn.Linear(100, 2*2*240)  \n        self.convt = nn.Sequential(\n            nn.ConvTranspose2d(240, 120, kernel_size=4, stride=2, padding=1, bias=False), # 4 x 4\n            nn.BatchNorm2d(120),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(120, 60, kernel_size=4, stride=2, padding=1, bias=False), # 8 x 8\n            nn.BatchNorm2d(60),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(60, 30, kernel_size=4, stride=2, padding=1, bias=False), # 16 x 16\n            nn.BatchNorm2d(30),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(30, 15, kernel_size=4, stride=2, padding=1, bias=False), # 32 x 32\n            nn.BatchNorm2d(15),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(15, 3, kernel_size=4, stride=2, padding=1, bias=False), # 64 x 64\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = x.view(-1, 240, 2, 2)\n        x = self.convt(x)\n        return x\n\n# Testing the ouput\nmodel = Generator()\noutput = model(torch.rand(size=(100,)))\nfirst_img = torch.transpose(output[0], 0, 2).detach().numpy()\nplt.imshow(first_img)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing generator, descriminator, and data loader. \ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbatch_size = 10\ngenerator = Generator().to(device)\ndescriminator = Descriminator().to(device)\ndataloader = torch.utils.data.DataLoader(paints, batch_size=batch_size, shuffle=True)     ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Loop   \ndef training_loop(gen_lr, des_lr, epochs):\n    \n    # Initializing the optimizers \n    gen_opt = torch.optim.AdamW(generator.parameters(), lr=gen_lr, betas=(0.5,0.999))\n    des_opt = torch.optim.AdamW(descriminator.parameters(), lr=des_lr, betas=(0.5,0.999))      \n        \n    for epoch in range(epochs):\n        g_loss_list = []\n        d_loss_list = []\n    \n        for i, batch in enumerate(dataloader, 0):\n            \n            # Training the descriminator on the real images\n            des_opt.zero_grad(set_to_none=True)\n            predict = descriminator(batch.float())\n            d_loss = F.binary_cross_entropy(predict, torch.ones((len(predict),1), device=device)) \n            d_loss.backward()\n            \n            # Training the descriminator on the fake images\n            rand_noise = torch.randn(batch_size, 100, device=device)\n            gen_img = generator(rand_noise)\n            predict = descriminator(gen_img.detach().to(device))\n            d_loss_fake = F.binary_cross_entropy(predict,torch.zeros((len(predict),1), device=device))\n            d_loss_fake.backward()\n            des_opt.step()\n        \n            # Training the generator on the descriminator's feedback\n            gen_opt.zero_grad(set_to_none=True)\n            predict = descriminator(gen_img)\n            g_loss = F.binary_cross_entropy(predict,torch.ones((len(predict),1), device=device))\n            g_loss.backward()\n            gen_opt.step()\n            \n            # Storing the losses\n            g_loss_list.append(g_loss)\n            d_loss_list.append(d_loss + d_loss_fake)\n\n        # Print the avg loss for the epoch\n        print('Gen Loss:', torch.mean(torch.stack(g_loss_list))) \n        print('Des Loss:', torch.mean(torch.stack(d_loss_list)))\n        \n        # Plot 10 fake apples\n        fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(10,1))\n        for i in range(10):\n            axes[i].axis('off')\n            axes[i].imshow(torch.transpose(gen_img[i], 0, 2).detach().numpy())\n        plt.show()\n    \ntraining_loop(.005, .005, 10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lowering the learning rate\ntraining_loop(.0001, .0001, 10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# More training\ntraining_loop(.0001, .0001, 10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results\n\nWhile the final generated images are clearly different from those in the dataset, there has been remarkable progress from the initial stages of the project. The model's evolution showcases its potential for creating diverse apple images with varying positions, colors, and lighting conditions.\n\nOne of the central challenges of this project was parameter tuning. Experimenting with different learning rates, discriminator dropout rates, and the number of layers in the generator model presented a complex challenge. A recurring issue was that the descriminator frequently became so effective at identifying fake apples that the generator was no longer able to learn from it.\n\nI decided to stop training the model at this point because the improvements were beginning to plateau, and I wanted the results to be easily reproducible without extensive computational resources. \n\nThis project serves as a demonstration of what a GAN can achieve with a relatively modest amount of computational power. Clearly, there is a lot of room for improvement. Using GPUs and distributed training would boost computational power and enable the model to become more complex with additional layers and larger image sizes, likely yielding more impressive results. ","metadata":{}}]}