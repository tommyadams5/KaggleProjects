{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tommyadams/taylor-swift-transformer-language-model?scriptVersionId=141675659\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"2e3e33f4","metadata":{"papermill":{"duration":0.008986,"end_time":"2023-09-01T18:34:36.486288","exception":false,"start_time":"2023-09-01T18:34:36.477302","status":"completed"},"tags":[]},"source":["# <center> Taylor Swift Transformer Language Model </center>\n","\n","<center><img src='https://media.glamour.com/photos/59df542c017bd0228acd8003/4:3/w_2455,h_1841,c_limit/TAYLOR-SWIFT.jpg' height=600px width=600px>\n","<img src='https://www.hearai.pl/post/13-slt/image5.png' height=600px width=600px></center>\n","\n","## Project Summary\n","\n","Since the publication of the 2017 paper \"Attention Is All You Need\", transformer language models have been shown to be very effective for a variety of natural language processing tasks and they have led to the creation of superior chat bots, such as Chat GPT. Transformer language models are a type of neural network that are used for natural language processing tasks such as machine translation, text summarization, and question answering. They are known for their ability to learn long-range dependencies between words, which makes them well-suited for tasks that require understanding the context of a sentence or paragraph. \n","\n","In this project, I will build a transformer language model in PyTorch to generate lyrics in the style of Taylor Swift. I will do this by training the model on a dataset of lyrics from Taylor Swift's songs. The model will learn the patterns of Taylor Swift's writing style, and it will be able to use this knowledge to generate new lyrics that are similar to hers.\n","\n","The project will be divided into the following steps:\n","\n","- Collect a dataset of lyrics from Taylor Swift's songs.\n","- Prepare the dataset for training.\n","- Build the transformer language model.\n","- Train the model.\n","- Evaluate the model's performance.\n","- Generate new lyrics using the model.\n","\n","## Model Architecture\n","\n","My model architecture will be based on the paper \"Attention Is All You Need\", which can be seen in the image above. However, there will be some differences between my model and the model in the paper:\n","\n","- My transformers will not have an encoder or cross-attention portions since I am not translating from one language to another like in the paper. As a result, my transformers will only perform masked self-attention.\n","- I chose to do the normalization phase before the multi-head attention and feed forward portions of the model, as opposed to doing it afterwards like in the paper. I wanted to reduce gradients created by the attention and feed forward layers in the hopes that it would make training more stable. \n","- The paper chose to use cosine and sine positional embeddings to teach the model to generalize for longer context lengths than it was trained on. I chose to use a simple ascending numbering of the positions since my model is much smaller and I do not expect comprehension of long context lengths to be a limiting factor.\n","- Because I am limited by the computational power of my cpu, I chose to create a model with less layers, smaller embedding groups, and smaller training batch sizes. Also, I chose to have the model predict the next character as opposed to predicting the next word, which reduced the vocabulary of the model  and its computational intensity. "]},{"cell_type":"code","execution_count":1,"id":"2ddeacc6","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:36.505096Z","iopub.status.busy":"2023-09-01T18:34:36.504542Z","iopub.status.idle":"2023-09-01T18:34:40.203346Z","shell.execute_reply":"2023-09-01T18:34:40.202242Z"},"papermill":{"duration":3.711709,"end_time":"2023-09-01T18:34:40.206327","exception":false,"start_time":"2023-09-01T18:34:36.494618","status":"completed"},"tags":[]},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import glob"]},{"cell_type":"code","execution_count":2,"id":"858d1249","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:40.225248Z","iopub.status.busy":"2023-09-01T18:34:40.224605Z","iopub.status.idle":"2023-09-01T18:34:40.331757Z","shell.execute_reply":"2023-09-01T18:34:40.330606Z"},"papermill":{"duration":0.119865,"end_time":"2023-09-01T18:34:40.334482","exception":false,"start_time":"2023-09-01T18:34:40.214617","status":"completed"},"tags":[]},"outputs":[],"source":["# Importing the csv files and concatenating the data\n","# Data set is from https://www.kaggle.com/datasets/thespacefreak/taylor-swift-song-lyrics-all-albums\n","path = '/kaggle/input/taylor-swift-song-lyrics-all-albums/'\n","csv_files = glob.glob(path + \"/*.csv\")\n","df_list = (pd.read_csv(i) for i in csv_files)\n","df = pd.concat(df_list, ignore_index=True)\n","lyrics = '\\n'.join(df.loc[:,'lyric']) "]},{"cell_type":"code","execution_count":3,"id":"f48ab54e","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:40.352925Z","iopub.status.busy":"2023-09-01T18:34:40.352514Z","iopub.status.idle":"2023-09-01T18:34:40.358107Z","shell.execute_reply":"2023-09-01T18:34:40.356978Z"},"papermill":{"duration":0.018838,"end_time":"2023-09-01T18:34:40.361505","exception":false,"start_time":"2023-09-01T18:34:40.342667","status":"completed"},"scrolled":true,"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Knew he was a killer first time that I saw him\n","Wondered how many girls he had loved and left haunted\n","But if he's a ghost, then I can be a phantom\n","Holdin' him for ransom, some\n","Some boys are tryin' too hard, he don't try at all though\n","Younger than my exes, but he act like such a man, so\n","I see nothing better, I keep him forever\n","Like a vendetta-ta\n","I, I, I see how this is gon' go\n","Touch me and you'll never be alone\n","I-Island breeze and lights down low\n","No one has to know\n","In the middle of the night, in m\n"]}],"source":["print(lyrics[:500])"]},{"cell_type":"code","execution_count":4,"id":"57d4a8cc","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:40.379841Z","iopub.status.busy":"2023-09-01T18:34:40.379456Z","iopub.status.idle":"2023-09-01T18:34:40.392329Z","shell.execute_reply":"2023-09-01T18:34:40.391533Z"},"papermill":{"duration":0.024617,"end_time":"2023-09-01T18:34:40.394406","exception":false,"start_time":"2023-09-01T18:34:40.369789","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["'\\n   ! \" & \\' ( ) , - . 0 1 2 3 4 5 6 7 8 9 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y [ ] a b c d e f g h i j k l m n o p q r s t u v w x y z | \\xa0 é í ï ó е \\u2005 \\u200b – — ‘ ’ ” … \\u205f'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# List of all unique characters\n","' '.join(sorted(set(lyrics)))"]},{"cell_type":"code","execution_count":5,"id":"64b5cf4b","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:40.414482Z","iopub.status.busy":"2023-09-01T18:34:40.414049Z","iopub.status.idle":"2023-09-01T18:34:40.46154Z","shell.execute_reply":"2023-09-01T18:34:40.460411Z"},"papermill":{"duration":0.060733,"end_time":"2023-09-01T18:34:40.463863","exception":false,"start_time":"2023-09-01T18:34:40.40313","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," !',.0123456789?ABCDEFGHIJKLMNOPQRSTUVWXYabcdefghijklmnopqrstuvwxyz…\n"]}],"source":["# Cleaning the file by removing/replacing unnecessary characters and removing sections that are not lyrics\n","replace_with_space = ['\\u2005', '\\u200b', '\\u205f', '\\xa0', '-']\n","replace_letters = {'í':'i', 'é':'e', 'ï':'i', 'ó':'o', ';':',', '‘':'\\'', '’':'\\'', ':':',', 'е':'e'} \n","remove_list = ['\\)', '\\(', '–','\"','”', '\"', '\\[.*\\]', '.*\\|.*', '—']\n","\n","cleaned_lyrics = lyrics\n","\n","for old, new in replace_letters.items():\n","    cleaned_lyrics = cleaned_lyrics.replace(old, new)\n","for string in remove_list:\n","    cleaned_lyrics = re.sub(string,'',cleaned_lyrics)\n","for string in replace_with_space:\n","    cleaned_lyrics = re.sub(string,' ',cleaned_lyrics)\n","print(''.join(sorted(set(cleaned_lyrics))))"]},{"cell_type":"code","execution_count":6,"id":"891c989b","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:40.483145Z","iopub.status.busy":"2023-09-01T18:34:40.482526Z","iopub.status.idle":"2023-09-01T18:34:40.488103Z","shell.execute_reply":"2023-09-01T18:34:40.486928Z"},"papermill":{"duration":0.017726,"end_time":"2023-09-01T18:34:40.49026","exception":false,"start_time":"2023-09-01T18:34:40.472534","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["296805 293121\n"]}],"source":["print(len(lyrics), len(cleaned_lyrics))"]},{"cell_type":"code","execution_count":7,"id":"dbd49c03","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:40.510849Z","iopub.status.busy":"2023-09-01T18:34:40.50995Z","iopub.status.idle":"2023-09-01T18:34:40.524681Z","shell.execute_reply":"2023-09-01T18:34:40.523483Z"},"papermill":{"duration":0.027021,"end_time":"2023-09-01T18:34:40.527197","exception":false,"start_time":"2023-09-01T18:34:40.500176","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["She's cheer captain\n"]}],"source":["# Creating an encoder and decoder to convert each character (char) to a number to feed into the model\n","vocab = sorted(set(cleaned_lyrics))\n","int_to_char = {int:char for int,char in enumerate(vocab)}\n","char_to_int = {char:int for int,char in enumerate(vocab)}\n","encoder = lambda string: [char_to_int[char] for char in string] \n","decoder = lambda list: ''.join([int_to_char[i] for i in list]) \n","\n","print(decoder(encoder(\"She's cheer captain\")))"]},{"cell_type":"code","execution_count":8,"id":"65666dfa","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:40.547102Z","iopub.status.busy":"2023-09-01T18:34:40.546419Z","iopub.status.idle":"2023-09-01T18:34:40.659738Z","shell.execute_reply":"2023-09-01T18:34:40.658568Z"},"papermill":{"duration":0.126734,"end_time":"2023-09-01T18:34:40.662764","exception":false,"start_time":"2023-09-01T18:34:40.53603","status":"completed"},"tags":[]},"outputs":[],"source":["# Setting aside a portion for training the model and a portion for testing the data to prevent the model from overfitting to the data it is tested on\n","lyric_tensor = torch.tensor(encoder(cleaned_lyrics), dtype=torch.long)\n","split_point = int(len(lyric_tensor)*0.9)\n","train = lyric_tensor[:split_point]\n","test = lyric_tensor[split_point:]"]},{"cell_type":"code","execution_count":9,"id":"ab227931","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:40.682757Z","iopub.status.busy":"2023-09-01T18:34:40.682317Z","iopub.status.idle":"2023-09-01T18:34:40.794511Z","shell.execute_reply":"2023-09-01T18:34:40.793394Z"},"papermill":{"duration":0.124853,"end_time":"2023-09-01T18:34:40.796847","exception":false,"start_time":"2023-09-01T18:34:40.671994","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[" prediction dimensions: torch.Size([20, 69])\n","\n"," context input: away\n","I should say, h\n","\n"," context + response: away\n","I should say, h.JH7trIIIL\n"," pn?TL00l4T0HaMBML\n","\n"]}],"source":["#Creating a basic language model with only an embedding matrix. This model only references the most recent char to generate the next char\n","class BasicModel(nn.Module):\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        self.char_embeddings = nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, context):\n","        predictions = self.char_embeddings(context)\n","        return predictions\n","\n","    def generate(self, context, length):\n","        for i in range(length):\n","            predictions = self(context)\n","            predictions = predictions[-1, :] # Only referencing most recent char\n","            probabilities = F.softmax(predictions, dim=-1) # Normalize across the embedding dimension (aka vocab_size) so that they all add up to 1.00\n","            next_char = torch.multinomial(probabilities, num_samples=1) # Samples randomly from the prob distribution of the embedding dimension\n","            context = torch.cat((context, next_char))\n","        return context\n","\n","# Selecting a random batch of text\n","torch.manual_seed(400)\n","vocab_size = len(set(cleaned_lyrics))\n","batch_size = 20\n","index = torch.randint(low=0, high=len(train) - batch_size, size=(1,))\n","context = train[index:(index+batch_size)]\n","\n","# Feeding the batch (context) into the model and asking it to generate text following it \n","model = BasicModel(vocab_size)\n","predictions = model(context)\n","print(' prediction dimensions:', predictions.shape) # Should have dimensions (batch_size, vocab_size)\n","print('\\n context input:', decoder(context.tolist())) # Context input\n","print('\\n context + response:', decoder( model.generate(context, length=30).tolist()))"]},{"cell_type":"code","execution_count":10,"id":"6f811a4c","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:40.817311Z","iopub.status.busy":"2023-09-01T18:34:40.816634Z","iopub.status.idle":"2023-09-01T18:34:40.833426Z","shell.execute_reply":"2023-09-01T18:34:40.832329Z"},"papermill":{"duration":0.029817,"end_time":"2023-09-01T18:34:40.835883","exception":false,"start_time":"2023-09-01T18:34:40.806066","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(tensor([[60,  1, 61, 49, 46,  1, 59, 46, 42, 60],\n","         [56, 61,  1, 43, 42, 45,  1, 43, 53, 56],\n","         [59, 46,  0, 41, 56, 62,  1, 62, 55, 45],\n","         [50, 60, 50, 43, 53, 46,  1, 60, 61, 59],\n","         [ 0, 41, 56, 62,  1, 52, 55, 46, 64,  1]]),\n"," tensor([[ 1, 61, 49, 46,  1, 59, 46, 42, 60, 56],\n","         [61,  1, 43, 42, 45,  1, 43, 53, 56, 56],\n","         [46,  0, 41, 56, 62,  1, 62, 55, 45, 46],\n","         [60, 50, 43, 53, 46,  1, 60, 61, 59, 50],\n","         [41, 56, 62,  1, 52, 55, 46, 64,  1, 66]]))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Adding ability for model to process multiple batches to improve training efficiency, and adding targets to measure loss\n","def create_batches(data, batch_size, batches):\n","    index = torch.randint(low=0, high=len(data) - batch_size, size=(batches,))\n","    context = torch.stack([data[row:(row+batch_size)] for row in index])\n","    target = torch.stack([data[row+1:(row+batch_size+1)] for row in index]) # Target is just the context shifted one char to the right\n","    return context, target\n","\n","create_batches(train, 10, 5)"]},{"cell_type":"code","execution_count":11,"id":"d157de90","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:40.856764Z","iopub.status.busy":"2023-09-01T18:34:40.855996Z","iopub.status.idle":"2023-09-01T18:34:40.919182Z","shell.execute_reply":"2023-09-01T18:34:40.91793Z"},"papermill":{"duration":0.076369,"end_time":"2023-09-01T18:34:40.921461","exception":false,"start_time":"2023-09-01T18:34:40.845092","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[" prediction dimensions: torch.Size([40, 69])\n","loss: tensor(4.8738, grad_fn=<NllLossBackward0>)\n","\n"," batch1:\n"," at you'll never findrJL\n","Un.YH7NO3f6iAy6aY?yREsE0Q4\n","\n"," batch2:\n"," e if I want to try aCMS5z!oYkH j1z.5FuKnflpr5slfH \n"]}],"source":["# Expanding the basic model so that it can handle multiple batches at the same time. Also, added ability for model to calculate the loss function (cross_entropy).\n","class BatchModel(nn.Module):\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        self.char_embeddings = nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, context, target=None):\n","        predictions = self.char_embeddings(context) # Context is the context output of create_batches and has dimensions (batches, batch_size)\n","\n","        if target == None:\n","            loss = None\n","        else:\n","            # Resizing the shapes of predictions and target to meet requirements for cross_entropy loss function\n","            A, B, C = predictions.shape\n","            predictions = predictions.view(A * B, C)\n","            target = target.view(A * B)\n","            loss = F.cross_entropy(predictions, target)\n","        \n","        return predictions, loss\n","\n","    def generate(self, context, length):\n","        for i in range(length):\n","            predictions, loss = self(context)\n","            predictions = predictions[:, -1, :] # Only referencing most recent char\n","            probabilities = F.softmax(predictions, dim=-1) # Scale data across the embedding dimension of vocab_size so that they all add up to 1.00\n","            next_char = torch.multinomial(probabilities, num_samples=1) # Samples randomly from the prob distribution of the embedding dimension\n","            context = torch.cat((context, next_char), dim=1)\n","        return context\n","\n","vocab_size = len(set(cleaned_lyrics))\n","context, target = create_batches(train, batch_size=20, batches=2)\n","model = BatchModel(vocab_size)\n","predictions, loss = model(context, target)\n","output = model.generate(context, length=30)\n","print(' prediction dimensions:', predictions.shape) # Should have dimensions (batches * batch_size, vocab_size)\n","print('loss:', loss)\n","print('\\n batch1:\\n', decoder(output[0].tolist()))\n","print('\\n batch2:\\n', decoder(output[1].tolist()))"]},{"cell_type":"code","execution_count":12,"id":"6029bee6","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:40.942402Z","iopub.status.busy":"2023-09-01T18:34:40.941736Z","iopub.status.idle":"2023-09-01T18:34:42.866316Z","shell.execute_reply":"2023-09-01T18:34:42.864985Z"},"papermill":{"duration":1.938298,"end_time":"2023-09-01T18:34:42.869161","exception":false,"start_time":"2023-09-01T18:34:40.930863","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["3.553962469100952\n","2.957204580307007\n","2.612980365753174\n","2.465458393096924\n","2.4347734451293945\n","2.439598321914673\n","2.463512659072876\n","2.4112160205841064\n","2.344435930252075\n","2.381838321685791\n","\n"," context + response:\n","  look what you made Sooqure\n","And witld ck, au'me clndasalit weVm8ryolyoftrcesickn's ake hath meto5Kay I anet widn apSou t'3d youromyooheadOowing ly wis y ysh\n","So I'th tit, \n"]}],"source":["# Using an optimizer to train the model. \n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n","for i in range(10):\n","    for i in range(100):\n","        context, target = create_batches(train, batch_size=20, batches=30)\n","        predictions, loss = model(context, target)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","    print(loss.item())\n","\n","print('\\n context + response:\\n', decoder( model.generate(context, length=150)[0].tolist()))"]},{"cell_type":"code","execution_count":13,"id":"3f9297d2","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:42.891872Z","iopub.status.busy":"2023-09-01T18:34:42.891261Z","iopub.status.idle":"2023-09-01T18:34:42.92171Z","shell.execute_reply":"2023-09-01T18:34:42.920437Z"},"papermill":{"duration":0.044748,"end_time":"2023-09-01T18:34:42.924277","exception":false,"start_time":"2023-09-01T18:34:42.879529","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([15, 30, 20])\n"]}],"source":["# Creating a head of attention so that the model can see past chars when predicting the next char\n","class Attention(nn.Module):\n","\n","    def __init__(self, batch_size, embed_groups, head_groups):\n","        super().__init__()\n","        self.query = nn.Linear(embed_groups, head_groups, bias=False) # Bias is set to false because a normalization layer follows\n","        self.key = nn.Linear(embed_groups, head_groups, bias=False)\n","        self.value = nn.Linear(embed_groups, head_groups, bias=False)\n","        self.register_buffer('mask', torch.tril(torch.ones(batch_size, batch_size))) \n","\n","    def forward(self, x):\n","        A,B,C = x.shape\n","        query = self.query(x)\n","        key = self.key(x)   \n","\n","        # This code results in dimensions of (A, B, B), which maps each char to each other char in the context. \n","        # The matrix is multiplied by 1 / (embed_groups)**0.5 to prevent the soft max layer from sharpening to much in response to high dot product values\n","        att = query @ key.transpose(-1,-2) * C**-0.5 \n","\n","        # Apply mask so that each char cannot 'see' future chars that come after it\n","        att = att.masked_fill(self.mask[:B, :B] == 0, float('-inf')) \n","\n","        # Scale data to be between 0 and 1\n","        att = F.softmax(att, dim=-1)\n","        \n","        value = self.value(x)\n","        output = att @ value # Results in dimensions of (A, B, head_groups)\n","        return output\n","\n","example = Attention(30, 60, 20)\n","input = torch.rand(size=(15, 30, 60))\n","print(example(input).shape)\n"]},{"cell_type":"code","execution_count":14,"id":"37aee65d","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:34:42.947922Z","iopub.status.busy":"2023-09-01T18:34:42.947518Z","iopub.status.idle":"2023-09-01T18:36:54.33138Z","shell.execute_reply":"2023-09-01T18:36:54.329204Z"},"papermill":{"duration":131.398269,"end_time":"2023-09-01T18:36:54.333871","exception":false,"start_time":"2023-09-01T18:34:42.935602","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["train loss: 2.325967788696289\n","test loss: 2.206486225128174\n","train loss: 2.240100145339966\n","test loss: 2.077044725418091\n","train loss: 2.1006884574890137\n","test loss: 2.1636483669281006\n","train loss: 1.9667949676513672\n","test loss: 1.9528011083602905\n","train loss: 1.9457050561904907\n","test loss: 1.7252171039581299\n","train loss: 1.7470109462738037\n","test loss: 1.8231513500213623\n","train loss: 1.647486686706543\n","test loss: 1.8626803159713745\n","train loss: 1.6320445537567139\n","test loss: 1.673216700553894\n","train loss: 1.427269458770752\n","test loss: 1.5662214756011963\n","train loss: 1.7723850011825562\n","test loss: 1.5276211500167847\n","\n"," Olly bruttery, brokem at sumpine the to the pitin the's life they shar\n","And when your night har, be walking mill the knews you\n","Lave my when, I'll her sain\n","Be your I knemst be you\n","But I leave of you\n","An't gonna creeGr I can creaching cars\n","And I dall you wisckito thirs brrong. was fholt\n","Can't it in thre\n"]}],"source":["# Creating multiple heads of attention, adding feed forward linear layers, stacking multiple transformers\n","\n","# Separated out the general parameters to make them easier to adjust\n","vocab_size = len(set(cleaned_lyrics))\n","batches = 15 \n","batch_size = 30 \n","embed_groups = 60\n","num_heads = 5\n","head_groups = embed_groups // num_heads\n","layers = 6  # Number of transformers\n","learning_rate = 1e-3\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","def create_batches(data, batch_size, batches):\n","    index = torch.randint(low=0, high=len(data) - batch_size, size=(batches,))\n","    context = torch.stack([data[row:(row+batch_size)] for row in index])\n","    target = torch.stack([data[row+1:(row+batch_size+1)] for row in index]) \n","    context, target = context.to(device), target.to(device) # Added ability to run on cuda\n","    return context, target\n","\n","class Attention(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.query = nn.Linear(embed_groups, head_groups, bias=False)\n","        self.key = nn.Linear(embed_groups, head_groups, bias=False)\n","        self.value = nn.Linear(embed_groups, head_groups, bias=False)\n","        self.register_buffer('mask', torch.tril(torch.ones(batch_size, batch_size)))\n","\n","    def forward(self, x):\n","        A,B,C = x.shape\n","        query = self.query(x)\n","        key = self.key(x)   \n","\n","        att = query @ key.transpose(-1,-2) * C**-0.5 \n","        att = att.masked_fill(self.mask[:B, :B] == 0, float('-inf')) \n","        att = F.softmax(att, dim=-1)\n","        \n","        value = self.value(x)\n","        output = att @ value\n","        return output\n","\n","class MultipleAttention(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.att_heads = nn.ModuleList([Attention() for i in range(num_heads)])\n","        self.att_reader = nn.Linear(embed_groups, embed_groups)\n","\n","    def forward(self, x):\n","        combined_att = torch.cat([i(x) for i in self.att_heads], dim=-1)\n","        output = self.att_reader(combined_att)\n","        return output\n","\n","class FeedFoward(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.ff_network = nn.Sequential(\n","            nn.Linear(embed_groups, 5 * embed_groups),\n","            nn.ReLU(),\n","            nn.Linear(5 * embed_groups, embed_groups))\n","    # ReLU is added because it is a non-linear function, which allows the model to learn more complex relationships\n","    \n","    def forward(self, x):\n","        return self.ff_network(x)\n","\n","class Transformer(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.matt = MultipleAttention()\n","        self.ff = FeedFoward()\n","        self.linear1 = nn.LayerNorm(embed_groups)\n","        self.linear2 = nn.LayerNorm(embed_groups)\n","\n","    def forward(self, x):\n","        # Residuals are added to prevent vanishing gradient\n","        x = x + self.matt(self.linear1(x)) \n","        x = x + self.ff(self.linear2(x)) \n","        return x\n","\n","class BatchModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.char_embeddings = nn.Embedding(vocab_size, embed_groups)\n","        self.pos_embeddings = nn.Embedding(batch_size, embed_groups) # Adds position embeddings to model can identify the order of the chars\n","        self.transformers = nn.Sequential(*[Transformer() for i in range(layers)])\n","        self.final_norm = nn.LayerNorm(embed_groups)\n","        self.final_linear = nn.Linear(embed_groups, vocab_size)\n","\n","\n","    def forward(self, context, target=None):\n","        A, B = context.shape\n","        full_embed = self.char_embeddings(context) + self.pos_embeddings(torch.arange(B, device=device))\n","        x = self.transformers(full_embed)\n","        x = self.final_norm(x)\n","        predictions = self.final_linear(x)\n","        \n","        \n","        if target == None:\n","            loss = None\n","        else:\n","            A, B, C = predictions.shape\n","            predictions = predictions.view(A * B, C)\n","            target = target.view(A * B)\n","            loss = F.cross_entropy(predictions, target)\n","        \n","        return predictions, loss\n","\n","    def generate(self, context, length):\n","        for i in range(length):\n","            short_context = context[:, -batch_size:] # Reduce context to only focus on last batch_size of chars because positions are embedded\n","            predictions, loss = self(short_context)\n","            predictions = predictions[:, -1, :] \n","            probabilities = F.softmax(predictions, dim=-1) \n","            next_char = torch.multinomial(probabilities, num_samples=1)\n","            context = torch.cat((context, next_char), dim=1)\n","        return context\n","\n","model = BatchModel()\n","model = model.to(device) # Added ability to run on cuda\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) # Lowered the learning rate\n","\n","# Training loop\n","for i in range(10):\n","    for j in range(200):\n","        context, target = create_batches(train, batch_size=batch_size, batches=batches)\n","        predictions, loss = model(context, target)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","    print('train loss:', loss.item())\n","\n","    # Added test loss calculation to look for overfitting\n","    context, target = create_batches(test, batch_size=batch_size, batches=batches)\n","    predictions, loss = model(context, target)\n","    print('test loss:', loss.item())\n","\n","print('\\n',decoder(model.generate(context, length=300)[0][batch_size:].tolist()))"]},{"cell_type":"code","execution_count":15,"id":"8d2f1569","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:36:54.360218Z","iopub.status.busy":"2023-09-01T18:36:54.359833Z","iopub.status.idle":"2023-09-01T18:39:05.079681Z","shell.execute_reply":"2023-09-01T18:39:05.078478Z"},"papermill":{"duration":130.735529,"end_time":"2023-09-01T18:39:05.082262","exception":false,"start_time":"2023-09-01T18:36:54.346733","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["train loss: 1.46900475025177\n","test loss: 1.6524217128753662\n","train loss: 1.3995614051818848\n","test loss: 1.489622950553894\n","train loss: 1.5939496755599976\n","test loss: 1.570263147354126\n","train loss: 1.534603238105774\n","test loss: 1.4992766380310059\n","train loss: 1.5643774271011353\n","test loss: 1.4966145753860474\n","train loss: 1.2557839155197144\n","test loss: 1.5761948823928833\n","train loss: 1.3989579677581787\n","test loss: 1.3395514488220215\n","train loss: 1.4174282550811768\n","test loss: 1.516241192817688\n","train loss: 1.2189892530441284\n","test loss: 1.4019126892089844\n","train loss: 1.4027663469314575\n","test loss: 1.3851622343063354\n","\n"," ame the begging, now, I do\n","And I love ever\n","Did will hav ai last the tagloi\n","The creecher e else anywhem mice\n","I nothow a forgume, wording you furnt\n","And all at your funt gorsen, when you go3\n","Do at you talk is couldn it old in a sirce\n","And I've never alright, with nights I might me suntil\n","Never like down\n"]}],"source":["# More training.\n","\n","for i in range(10):\n","    for j in range(200):\n","        context, target = create_batches(train, batch_size=batch_size, batches=batches)\n","        predictions, loss = model(context, target)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","    print('train loss:', loss.item())\n","\n","    context, target = create_batches(test, batch_size=batch_size, batches=batches)\n","    predictions, loss = model(context, target)\n","    print('test loss:', loss.item())\n","\n","print('\\n',decoder(model.generate(context, length=300)[0][batch_size:].tolist()))"]},{"cell_type":"code","execution_count":16,"id":"a19eaf8e","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:39:05.110186Z","iopub.status.busy":"2023-09-01T18:39:05.10979Z","iopub.status.idle":"2023-09-01T18:41:14.830613Z","shell.execute_reply":"2023-09-01T18:41:14.8286Z"},"papermill":{"duration":129.737779,"end_time":"2023-09-01T18:41:14.833147","exception":false,"start_time":"2023-09-01T18:39:05.095368","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["train loss: 1.3376103639602661\n","test loss: 1.6179249286651611\n","train loss: 1.3626505136489868\n","test loss: 1.3440215587615967\n","train loss: 1.4894189834594727\n","test loss: 1.5395950078964233\n","train loss: 1.1720752716064453\n","test loss: 1.6247557401657104\n","train loss: 1.2934796810150146\n","test loss: 1.4623098373413086\n","train loss: 1.402515172958374\n","test loss: 1.5500829219818115\n","train loss: 1.3286421298980713\n","test loss: 1.7675071954727173\n","train loss: 1.2494961023330688\n","test loss: 1.56464684009552\n","train loss: 1.2936402559280396\n","test loss: 1.4446462392807007\n","train loss: 1.208522081375122\n","test loss: 1.5539568662643433\n","\n"," shame\n","I should've fake, never\n","'Cause my around of the sed out\n","And one to the baci tch his you think this is the lits exies the side\n","I only through me night to had to do\n","That was night at you're me the porr yoles spolet dip\n","And it easy apar the nighter the part, he we'll never things in you combatte \n"]}],"source":["# More training with reduced learning rate\n","learning_rate = 3e-4\n","\n","for i in range(10):\n","    for j in range(200):\n","        context, target = create_batches(train, batch_size=batch_size, batches=batches)\n","        predictions, loss = model(context, target)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","    print('train loss:', loss.item())\n","\n","    context, target = create_batches(test, batch_size=batch_size, batches=batches)\n","    predictions, loss = model(context, target)\n","    print('test loss:', loss.item())\n","\n","print('\\n',decoder(model.generate(context, length=300)[0][batch_size:].tolist()))"]},{"cell_type":"code","execution_count":17,"id":"d01b5894","metadata":{"execution":{"iopub.execute_input":"2023-09-01T18:41:14.864259Z","iopub.status.busy":"2023-09-01T18:41:14.863872Z","iopub.status.idle":"2023-09-01T18:41:46.566008Z","shell.execute_reply":"2023-09-01T18:41:46.565131Z"},"papermill":{"duration":31.735725,"end_time":"2023-09-01T18:41:46.583839","exception":false,"start_time":"2023-09-01T18:41:14.848114","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," nge\n","Pats there I sun hate up lican pernectestly\n","Or he's fake a spoling of in the pict\n","That this perfel the bother our are to killo\n","What gue and scorrard it's for faithy\n","I'm the fake a shaga lin't wanna be with bate\n","I found at turn you\n","I hope it's a should, shake me thinnu\n","But the marver the prines the one you breeathe the again, and I hush, I shake you thoughters and pasken out and me\n","And when I know is make me shade\n","Like the one up and I found anyworing\n","Ssead, I'll never the freemore\n","It's a little here here phone hunt, but got lost should\n","And why I'm all where moner\n","If I could've been a each now a don't know that like bloods up, what I say Oh\n","Oh, oh, oh oh, oh\n","Wholed it's fr?\n","Knew you prozin with home\n","I had to do 'ema\n",".., his love man, baby, no get me\n","When the do way I didn't do\n","I could dending up come backst home\n","For a saw you make\n","She's sure, nothing their again\n","In I hidn't say what you wanted and rasteer of the ewould fakes back our penterfurade\n","OnlBy the hand to desk of of the neard in the boked of a raint for the perst on turns\n","On the I remember know you and me\n","Or and closet, I face\n","Every sturt in twices love\n","Piep there what a pefore hung\n","You're all heard the night\n","When Sromed you, I know you not heart of look\n","I runninned this just should phoh plackes in rominne\n","It let you should've never never head\n","I'm show it all I had real out through\n","You almove think over and movies\n","That's makil if when it's not I'm deferfed you mouse\n","But this best me stoleb\n","Oh, day, I have to a go,\n"]}],"source":["# Printing a longer generation output from the model\n","print('\\n',decoder(model.generate(context, length=1500)[1][batch_size:].tolist()))"]},{"cell_type":"markdown","id":"186b7f48","metadata":{"papermill":{"duration":0.013486,"end_time":"2023-09-01T18:41:46.611066","exception":false,"start_time":"2023-09-01T18:41:46.59758","status":"completed"},"tags":[]},"source":["## Results\n","\n","Although the final output of the model sounds like gibberish, it greatly improved from its initial state where it vomited random characters. The model is able to produce real words, which is impressive given that it is only trained to predict the next character. The model has also learned to capitalize the first word of each line, and produces lines of lyrics that are on average the same length as the lyrics in the data set. I elected to stop training at this point because the model test error is only making small improvements with each iteration. \n","\n","This model shows the limitations of creating language models with a relatively small amount of compute power. Clearly, there is a lot of room for the model to improve. Using GPUs and distributed training would boost computational power and enable the model to become more complex with additional transformers, more embedding groups, and longer context sizes. Towards the end of the training, the model also started to suffer from overfitting, since the testing error was consistently higher than the training set error. To help reduce overfitting, I could introduce a dropping layer that would randomly drop some weights from the model; however, this would also significantly increase the training time to convergence. \n","\n","In conclusion, here are some of the more humorous lines from the final output:\n","\n","\"I could see things offside you\"\n","\n","\"I hope I still real in the way home\"\n","\n","\"I'm along your face, love lost with you\"\n","\n","\"I'm walk you here?\"\n","\n","\"This walling was dang of you\"\n","\n","\"And I just faked stray\"\n","\n","\"One the us lovers\"\n","\n","\"This is the sames, sing all as why\"\n"]}],"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":443.396036,"end_time":"2023-09-01T18:41:47.94951","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-09-01T18:34:24.553474","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}