{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tommyadams/taylor-swift-transformer-language-model?scriptVersionId=153510027\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"2a13dc79","metadata":{"papermill":{"duration":0.008012,"end_time":"2023-12-04T04:23:27.686157","exception":false,"start_time":"2023-12-04T04:23:27.678145","status":"completed"},"tags":[]},"source":["# <center> Taylor Swift Transformer Language Model </center>\n","\n","<center><img src='https://media.glamour.com/photos/59df542c017bd0228acd8003/4:3/w_2455,h_1841,c_limit/TAYLOR-SWIFT.jpg' height=600px width=600px>\n","<img src='https://daleonai.com/images/screen-shot-2021-05-06-at-12.12.21-pm.png' height=500px width=500px></center>\n","\n","## Project Summary\n","\n","Since the publication of the 2017 paper \"Attention Is All You Need\", transformer language models have been shown to be very effective for a variety of natural language processing tasks and they have led to the creation of superior chat bots, such as Chat GPT. Transformer language models are a type of neural network that are used for natural language processing tasks such as machine translation, text summarization, and question answering. They are known for their ability to learn long-range dependencies between words, which makes them well-suited for tasks that require understanding the context of a sentence or paragraph. \n","\n","In this project, I will build a transformer language model in PyTorch to generate lyrics in the style of Taylor Swift. I will do this by training the model on a dataset of lyrics from Taylor Swift's songs. The model will learn the patterns of Taylor Swift's writing style, and it will be able to use this knowledge to generate new lyrics that are similar to hers.\n","\n","The project will be divided into the following steps:\n","\n","- Collect a dataset of lyrics from Taylor Swift's songs.\n","- Prepare the dataset for training.\n","- Build the transformer language model.\n","- Train the model.\n","- Evaluate the model's performance.\n","- Generate new lyrics using the model.\n","\n","## Model Architecture\n","\n","My model architecture will be based on the paper \"Attention Is All You Need\", which can be seen in the image above. However, there will be some differences between my model and the model in the paper:\n","\n","- My transformers will not have an encoder or cross-attention portions since I am not translating from one language to another like in the paper. As a result, my transformers will only perform masked self-attention.\n","- I chose to do the normalization phase before the multi-head attention and feed forward portions of the model, as opposed to doing it afterwards like in the paper. I wanted to reduce gradients created by the attention and feed forward layers in the hopes that it would make training more stable. \n","- The paper chose to use cosine and sine positional embeddings to teach the model to generalize for longer context lengths than it was trained on. I chose to use a simple ascending numbering of the positions since my model is much smaller and I do not expect comprehension of long context lengths to be a limiting factor.\n","- Because I am limited by the computational power of my cpu, I chose to create a model with less layers, smaller embedding groups, and smaller training batch sizes. Also, I chose to have the model predict the next character as opposed to predicting the next word, which reduced the vocabulary of the model  and its computational intensity. "]},{"cell_type":"code","execution_count":1,"id":"00be3119","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:27.698596Z","iopub.status.busy":"2023-12-04T04:23:27.698002Z","iopub.status.idle":"2023-12-04T04:23:30.990591Z","shell.execute_reply":"2023-12-04T04:23:30.989722Z"},"papermill":{"duration":3.301466,"end_time":"2023-12-04T04:23:30.993002","exception":false,"start_time":"2023-12-04T04:23:27.691536","status":"completed"},"tags":[]},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import glob"]},{"cell_type":"code","execution_count":2,"id":"7feab9f9","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:31.00525Z","iopub.status.busy":"2023-12-04T04:23:31.004683Z","iopub.status.idle":"2023-12-04T04:23:31.105347Z","shell.execute_reply":"2023-12-04T04:23:31.104387Z"},"papermill":{"duration":0.109056,"end_time":"2023-12-04T04:23:31.107601","exception":false,"start_time":"2023-12-04T04:23:30.998545","status":"completed"},"tags":[]},"outputs":[],"source":["# Importing the csv files and concatenating the data\n","# Data set is from https://www.kaggle.com/datasets/thespacefreak/taylor-swift-song-lyrics-all-albums\n","path = '/kaggle/input/taylor-swift-song-lyrics-all-albums/'\n","csv_files = glob.glob(path + \"/*.csv\")\n","df_list = (pd.read_csv(i) for i in csv_files)\n","df = pd.concat(df_list, ignore_index=True)\n","lyrics = '\\n'.join(df.loc[:,'lyric']) "]},{"cell_type":"code","execution_count":3,"id":"ca85412e","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:31.119179Z","iopub.status.busy":"2023-12-04T04:23:31.118885Z","iopub.status.idle":"2023-12-04T04:23:31.12376Z","shell.execute_reply":"2023-12-04T04:23:31.122907Z"},"papermill":{"duration":0.013292,"end_time":"2023-12-04T04:23:31.126236","exception":false,"start_time":"2023-12-04T04:23:31.112944","status":"completed"},"scrolled":true,"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Knew he was a killer first time that I saw him\n","Wondered how many girls he had loved and left haunted\n","But if he's a ghost, then I can be a phantom\n","Holdin' him for ransom, some\n","Some boys are tryin' too hard, he don't try at all though\n","Younger than my exes, but he act like such a man, so\n","I see nothing better, I keep him forever\n","Like a vendetta-ta\n","I, I, I see how this is gon' go\n","Touch me and you'll never be alone\n","I-Island breeze and lights down low\n","No one has to know\n","In the middle of the night, in m\n"]}],"source":["print(lyrics[:500])"]},{"cell_type":"code","execution_count":4,"id":"e671750b","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:31.137795Z","iopub.status.busy":"2023-12-04T04:23:31.137539Z","iopub.status.idle":"2023-12-04T04:23:31.148798Z","shell.execute_reply":"2023-12-04T04:23:31.147916Z"},"papermill":{"duration":0.019302,"end_time":"2023-12-04T04:23:31.150832","exception":false,"start_time":"2023-12-04T04:23:31.13153","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["'\\n   ! \" & \\' ( ) , - . 0 1 2 3 4 5 6 7 8 9 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y [ ] a b c d e f g h i j k l m n o p q r s t u v w x y z | \\xa0 é í ï ó е \\u2005 \\u200b – — ‘ ’ ” … \\u205f'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# List of all unique characters\n","' '.join(sorted(set(lyrics)))"]},{"cell_type":"code","execution_count":5,"id":"61bc7de5","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:31.16279Z","iopub.status.busy":"2023-12-04T04:23:31.162524Z","iopub.status.idle":"2023-12-04T04:23:31.203629Z","shell.execute_reply":"2023-12-04T04:23:31.202621Z"},"papermill":{"duration":0.049319,"end_time":"2023-12-04T04:23:31.205549","exception":false,"start_time":"2023-12-04T04:23:31.15623","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," !',.0123456789?ABCDEFGHIJKLMNOPQRSTUVWXYabcdefghijklmnopqrstuvwxyz…\n"]}],"source":["# Cleaning the file by removing/replacing unnecessary characters and removing sections that are not lyrics\n","replace_with_space = ['\\u2005', '\\u200b', '\\u205f', '\\xa0', '-']\n","replace_letters = {'í':'i', 'é':'e', 'ï':'i', 'ó':'o', ';':',', '‘':'\\'', '’':'\\'', ':':',', 'е':'e'} \n","remove_list = ['\\)', '\\(', '–','\"','”', '\"', '\\[.*\\]', '.*\\|.*', '—']\n","\n","cleaned_lyrics = lyrics\n","\n","for old, new in replace_letters.items():\n","    cleaned_lyrics = cleaned_lyrics.replace(old, new)\n","for string in remove_list:\n","    cleaned_lyrics = re.sub(string,'',cleaned_lyrics)\n","for string in replace_with_space:\n","    cleaned_lyrics = re.sub(string,' ',cleaned_lyrics)\n","print(''.join(sorted(set(cleaned_lyrics))))"]},{"cell_type":"code","execution_count":6,"id":"6b1ab5a8","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:31.217391Z","iopub.status.busy":"2023-12-04T04:23:31.217134Z","iopub.status.idle":"2023-12-04T04:23:31.222066Z","shell.execute_reply":"2023-12-04T04:23:31.221131Z"},"papermill":{"duration":0.012848,"end_time":"2023-12-04T04:23:31.22392","exception":false,"start_time":"2023-12-04T04:23:31.211072","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["296805 293121\n"]}],"source":["print(len(lyrics), len(cleaned_lyrics))"]},{"cell_type":"code","execution_count":7,"id":"a386a810","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:31.236226Z","iopub.status.busy":"2023-12-04T04:23:31.235939Z","iopub.status.idle":"2023-12-04T04:23:31.247601Z","shell.execute_reply":"2023-12-04T04:23:31.246555Z"},"papermill":{"duration":0.020117,"end_time":"2023-12-04T04:23:31.249619","exception":false,"start_time":"2023-12-04T04:23:31.229502","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["She's cheer captain\n"]}],"source":["# Creating an encoder and decoder to convert each character (char) to a number to feed into the model\n","vocab = sorted(set(cleaned_lyrics))\n","int_to_char = {int:char for int,char in enumerate(vocab)}\n","char_to_int = {char:int for int,char in enumerate(vocab)}\n","encoder = lambda string: [char_to_int[char] for char in string] \n","decoder = lambda list: ''.join([int_to_char[i] for i in list]) \n","\n","print(decoder(encoder(\"She's cheer captain\")))"]},{"cell_type":"code","execution_count":8,"id":"243a7adf","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:31.262285Z","iopub.status.busy":"2023-12-04T04:23:31.262022Z","iopub.status.idle":"2023-12-04T04:23:31.354694Z","shell.execute_reply":"2023-12-04T04:23:31.353721Z"},"papermill":{"duration":0.101466,"end_time":"2023-12-04T04:23:31.356963","exception":false,"start_time":"2023-12-04T04:23:31.255497","status":"completed"},"tags":[]},"outputs":[],"source":["# Setting aside a portion for training the model and a portion for testing the data to prevent the model from overfitting to the data it is tested on\n","lyric_tensor = torch.tensor(encoder(cleaned_lyrics), dtype=torch.long)\n","split_point = int(len(lyric_tensor)*0.9)\n","train = lyric_tensor[:split_point]\n","test = lyric_tensor[split_point:]"]},{"cell_type":"code","execution_count":9,"id":"d7c95f1b","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:31.369834Z","iopub.status.busy":"2023-12-04T04:23:31.369554Z","iopub.status.idle":"2023-12-04T04:23:31.487743Z","shell.execute_reply":"2023-12-04T04:23:31.486703Z"},"papermill":{"duration":0.126879,"end_time":"2023-12-04T04:23:31.489797","exception":false,"start_time":"2023-12-04T04:23:31.362918","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[" prediction dimensions: torch.Size([20, 69])\n","\n"," context input: away\n","I should say, h\n","\n"," context + response: away\n","I should say, h.JH7trIIIL\n"," pn?TL00l4T0HaMBML\n","\n"]}],"source":["#Creating a basic language model with only an embedding matrix. This model only references the most recent char to generate the next char\n","class BasicModel(nn.Module):\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        self.char_embeddings = nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, context):\n","        predictions = self.char_embeddings(context)\n","        return predictions\n","\n","    def generate(self, context, length):\n","        for i in range(length):\n","            predictions = self(context)\n","            predictions = predictions[-1, :] # Only referencing most recent char\n","            probabilities = F.softmax(predictions, dim=-1) # Normalize across the embedding dimension (aka vocab_size) so that they all add up to 1.00\n","            next_char = torch.multinomial(probabilities, num_samples=1) # Samples randomly from the prob distribution of the embedding dimension\n","            context = torch.cat((context, next_char))\n","        return context\n","\n","# Selecting a random batch of text\n","torch.manual_seed(400)\n","vocab_size = len(set(cleaned_lyrics))\n","batch_size = 20\n","index = torch.randint(low=0, high=len(train) - batch_size, size=(1,))\n","context = train[index:(index+batch_size)]\n","\n","# Feeding the batch (context) into the model and asking it to generate text following it \n","model = BasicModel(vocab_size)\n","predictions = model(context)\n","print(' prediction dimensions:', predictions.shape) # Should have dimensions (batch_size, vocab_size)\n","print('\\n context input:', decoder(context.tolist())) # Context input\n","print('\\n context + response:', decoder( model.generate(context, length=30).tolist()))"]},{"cell_type":"code","execution_count":10,"id":"d7dc87ae","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:31.503331Z","iopub.status.busy":"2023-12-04T04:23:31.503052Z","iopub.status.idle":"2023-12-04T04:23:31.5152Z","shell.execute_reply":"2023-12-04T04:23:31.514348Z"},"papermill":{"duration":0.021006,"end_time":"2023-12-04T04:23:31.517119","exception":false,"start_time":"2023-12-04T04:23:31.496113","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(tensor([[60,  1, 61, 49, 46,  1, 59, 46, 42, 60],\n","         [56, 61,  1, 43, 42, 45,  1, 43, 53, 56],\n","         [59, 46,  0, 41, 56, 62,  1, 62, 55, 45],\n","         [50, 60, 50, 43, 53, 46,  1, 60, 61, 59],\n","         [ 0, 41, 56, 62,  1, 52, 55, 46, 64,  1]]),\n"," tensor([[ 1, 61, 49, 46,  1, 59, 46, 42, 60, 56],\n","         [61,  1, 43, 42, 45,  1, 43, 53, 56, 56],\n","         [46,  0, 41, 56, 62,  1, 62, 55, 45, 46],\n","         [60, 50, 43, 53, 46,  1, 60, 61, 59, 50],\n","         [41, 56, 62,  1, 52, 55, 46, 64,  1, 66]]))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Adding ability for model to process multiple batches to improve training efficiency, and adding targets to measure loss\n","def create_batches(data, batch_size, batches):\n","    index = torch.randint(low=0, high=len(data) - batch_size, size=(batches,))\n","    context = torch.stack([data[row:(row+batch_size)] for row in index])\n","    target = torch.stack([data[row+1:(row+batch_size+1)] for row in index]) # Target is just the context shifted one char to the right\n","    return context, target\n","\n","create_batches(train, 10, 5)"]},{"cell_type":"code","execution_count":11,"id":"360c3811","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:31.530381Z","iopub.status.busy":"2023-12-04T04:23:31.530117Z","iopub.status.idle":"2023-12-04T04:23:31.578627Z","shell.execute_reply":"2023-12-04T04:23:31.577663Z"},"papermill":{"duration":0.057366,"end_time":"2023-12-04T04:23:31.580531","exception":false,"start_time":"2023-12-04T04:23:31.523165","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[" prediction dimensions: torch.Size([40, 69])\n","loss: tensor(4.8738, grad_fn=<NllLossBackward0>)\n","\n"," batch1:\n"," at you'll never findrJL\n","Un.YH7NO3f6iAy6aY?yREsE0Q4\n","\n"," batch2:\n"," e if I want to try aCMS5z!oYkH j1z.5FuKnflpr5slfH \n"]}],"source":["# Expanding the basic model so that it can handle multiple batches at the same time. Also, added ability for model to calculate the loss function (cross_entropy).\n","class BatchModel(nn.Module):\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        self.char_embeddings = nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, context, target=None):\n","        predictions = self.char_embeddings(context) # Context is the context output of create_batches and has dimensions (batches, batch_size)\n","\n","        if target == None:\n","            loss = None\n","        else:\n","            # Resizing the shapes of predictions and target to meet requirements for cross_entropy loss function\n","            A, B, C = predictions.shape\n","            predictions = predictions.view(A * B, C)\n","            target = target.view(A * B)\n","            loss = F.cross_entropy(predictions, target)\n","        \n","        return predictions, loss\n","\n","    def generate(self, context, length):\n","        for i in range(length):\n","            predictions, loss = self(context)\n","            predictions = predictions[:, -1, :] # Only referencing most recent char\n","            probabilities = F.softmax(predictions, dim=-1) # Scale data across the embedding dimension of vocab_size so that they all add up to 1.00\n","            next_char = torch.multinomial(probabilities, num_samples=1) # Samples randomly from the prob distribution of the embedding dimension\n","            context = torch.cat((context, next_char), dim=1)\n","        return context\n","\n","vocab_size = len(set(cleaned_lyrics))\n","context, target = create_batches(train, batch_size=20, batches=2)\n","model = BatchModel(vocab_size)\n","predictions, loss = model(context, target)\n","output = model.generate(context, length=30)\n","print(' prediction dimensions:', predictions.shape) # Should have dimensions (batches * batch_size, vocab_size)\n","print('loss:', loss)\n","print('\\n batch1:\\n', decoder(output[0].tolist()))\n","print('\\n batch2:\\n', decoder(output[1].tolist()))"]},{"cell_type":"code","execution_count":12,"id":"9ab55251","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:31.594189Z","iopub.status.busy":"2023-12-04T04:23:31.593916Z","iopub.status.idle":"2023-12-04T04:23:33.462636Z","shell.execute_reply":"2023-12-04T04:23:33.461524Z"},"papermill":{"duration":1.877747,"end_time":"2023-12-04T04:23:33.464597","exception":false,"start_time":"2023-12-04T04:23:31.58685","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["3.553962469100952\n","2.957204580307007\n","2.612980365753174\n","2.465458393096924\n","2.4347734451293945\n","2.439598321914673\n","2.463512659072876\n","2.4112160205841064\n","2.344435930252075\n","2.381838321685791\n","\n"," context + response:\n","  look what you made Sooqure\n","And witld ck, au'me clndasalit weVm8ryolyoftrcesickn's ake hath meto5Kay I anet widn apSou t'3d youromyooheadOowing ly wis y ysh\n","So I'th tit, \n"]}],"source":["# Using an optimizer to train the model. \n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n","for i in range(10):\n","    for i in range(100):\n","        context, target = create_batches(train, batch_size=20, batches=30)\n","        predictions, loss = model(context, target)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","    print(loss.item())\n","\n","print('\\n context + response:\\n', decoder( model.generate(context, length=150)[0].tolist()))"]},{"cell_type":"code","execution_count":13,"id":"801d9593","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:33.480766Z","iopub.status.busy":"2023-12-04T04:23:33.480455Z","iopub.status.idle":"2023-12-04T04:23:33.511787Z","shell.execute_reply":"2023-12-04T04:23:33.51078Z"},"papermill":{"duration":0.041812,"end_time":"2023-12-04T04:23:33.513952","exception":false,"start_time":"2023-12-04T04:23:33.47214","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([15, 30, 20])\n"]}],"source":["# Creating a head of attention so that the model can see past chars when predicting the next char\n","class Attention(nn.Module):\n","\n","    def __init__(self, batch_size, embed_groups, head_groups):\n","        super().__init__()\n","        self.query = nn.Linear(embed_groups, head_groups, bias=False) # Bias is set to false because a normalization layer follows\n","        self.key = nn.Linear(embed_groups, head_groups, bias=False)\n","        self.value = nn.Linear(embed_groups, head_groups, bias=False)\n","        self.register_buffer('mask', torch.tril(torch.ones(batch_size, batch_size))) \n","\n","    def forward(self, x):\n","        A,B,C = x.shape\n","        query = self.query(x)\n","        key = self.key(x)   \n","\n","        # This code results in dimensions of (A, B, B), which maps each char to each other char in the context. \n","        # The matrix is multiplied by 1 / (embed_groups)**0.5 to prevent the soft max layer from sharpening to much in response to high dot product values\n","        att = query @ key.transpose(-1,-2) * C**-0.5 \n","\n","        # Apply mask so that each char cannot 'see' future chars that come after it\n","        att = att.masked_fill(self.mask[:B, :B] == 0, float('-inf')) \n","\n","        # Scale data to be between 0 and 1\n","        att = F.softmax(att, dim=-1)\n","        \n","        value = self.value(x)\n","        output = att @ value # Results in dimensions of (A, B, head_groups)\n","        return output\n","\n","example = Attention(30, 60, 20)\n","input = torch.rand(size=(15, 30, 60))\n","print(example(input).shape)\n"]},{"cell_type":"code","execution_count":14,"id":"1f2f8ea8","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:23:33.529655Z","iopub.status.busy":"2023-12-04T04:23:33.529372Z","iopub.status.idle":"2023-12-04T04:24:42.400504Z","shell.execute_reply":"2023-12-04T04:24:42.399407Z"},"papermill":{"duration":68.88149,"end_time":"2023-12-04T04:24:42.402683","exception":false,"start_time":"2023-12-04T04:23:33.521193","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["train loss: 2.326132297515869\n","test loss: 2.205993890762329\n","train loss: 2.2401444911956787\n","test loss: 2.077082633972168\n","train loss: 2.1021549701690674\n","test loss: 2.166832208633423\n","train loss: 1.9675167798995972\n","test loss: 1.9544931650161743\n","train loss: 1.9489340782165527\n","test loss: 1.721160888671875\n","train loss: 1.7434898614883423\n","test loss: 1.8203922510147095\n","train loss: 1.6465755701065063\n","test loss: 1.8698229789733887\n","train loss: 1.639026165008545\n","test loss: 1.6773635149002075\n","train loss: 1.4218274354934692\n","test loss: 1.5577216148376465\n","train loss: 1.7636111974716187\n","test loss: 1.5312517881393433\n","\n"," Miss and staysking they hellsoret\n","And you that you ou hope\n","At leave is the dost you, Ind Twaiss were they sond this wisece, liSone\n","Tall therrer like as looke in at in Orneel, her my edensing\n","Wreoted to 2nd phast lest rebled in we lied, throws in the nothis\n","'Cause'ss noget us is around on I do that y\n"]}],"source":["# Creating multiple heads of attention, adding feed forward linear layers, stacking multiple transformers\n","\n","# Separated out the general parameters to make them easier to adjust\n","vocab_size = len(set(cleaned_lyrics))\n","batches = 15 \n","batch_size = 30 \n","embed_groups = 60\n","num_heads = 5\n","head_groups = embed_groups // num_heads\n","layers = 6  # Number of transformers\n","learning_rate = 1e-3\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","def create_batches(data, batch_size, batches):\n","    index = torch.randint(low=0, high=len(data) - batch_size, size=(batches,))\n","    context = torch.stack([data[row:(row+batch_size)] for row in index])\n","    target = torch.stack([data[row+1:(row+batch_size+1)] for row in index]) \n","    context, target = context.to(device), target.to(device) # Added ability to run on cuda\n","    return context, target\n","\n","class Attention(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.query = nn.Linear(embed_groups, head_groups, bias=False)\n","        self.key = nn.Linear(embed_groups, head_groups, bias=False)\n","        self.value = nn.Linear(embed_groups, head_groups, bias=False)\n","        self.register_buffer('mask', torch.tril(torch.ones(batch_size, batch_size)))\n","\n","    def forward(self, x):\n","        A,B,C = x.shape\n","        query = self.query(x)\n","        key = self.key(x)   \n","\n","        att = query @ key.transpose(-1,-2) * C**-0.5 \n","        att = att.masked_fill(self.mask[:B, :B] == 0, float('-inf')) \n","        att = F.softmax(att, dim=-1)\n","        \n","        value = self.value(x)\n","        output = att @ value\n","        return output\n","\n","class MultipleAttention(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.att_heads = nn.ModuleList([Attention() for i in range(num_heads)])\n","        self.att_reader = nn.Linear(embed_groups, embed_groups)\n","\n","    def forward(self, x):\n","        combined_att = torch.cat([i(x) for i in self.att_heads], dim=-1)\n","        output = self.att_reader(combined_att)\n","        return output\n","\n","class FeedFoward(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.ff_network = nn.Sequential(\n","            nn.Linear(embed_groups, 5 * embed_groups),\n","            nn.ReLU(),\n","            nn.Linear(5 * embed_groups, embed_groups))\n","    # ReLU is added because it is a non-linear function, which allows the model to learn more complex relationships\n","    \n","    def forward(self, x):\n","        return self.ff_network(x)\n","\n","class Transformer(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.matt = MultipleAttention()\n","        self.ff = FeedFoward()\n","        self.linear1 = nn.LayerNorm(embed_groups)\n","        self.linear2 = nn.LayerNorm(embed_groups)\n","\n","    def forward(self, x):\n","        # Residuals are added to prevent vanishing gradient\n","        x = x + self.matt(self.linear1(x)) \n","        x = x + self.ff(self.linear2(x)) \n","        return x\n","\n","class BatchModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.char_embeddings = nn.Embedding(vocab_size, embed_groups)\n","        self.pos_embeddings = nn.Embedding(batch_size, embed_groups) # Adds position embeddings to model can identify the order of the chars\n","        self.transformers = nn.Sequential(*[Transformer() for i in range(layers)])\n","        self.final_norm = nn.LayerNorm(embed_groups)\n","        self.final_linear = nn.Linear(embed_groups, vocab_size)\n","\n","\n","    def forward(self, context, target=None):\n","        A, B = context.shape\n","        full_embed = self.char_embeddings(context) + self.pos_embeddings(torch.arange(B, device=device))\n","        x = self.transformers(full_embed)\n","        x = self.final_norm(x)\n","        predictions = self.final_linear(x)\n","        \n","        \n","        if target == None:\n","            loss = None\n","        else:\n","            A, B, C = predictions.shape\n","            predictions = predictions.view(A * B, C)\n","            target = target.view(A * B)\n","            loss = F.cross_entropy(predictions, target)\n","        \n","        return predictions, loss\n","\n","    def generate(self, context, length):\n","        for i in range(length):\n","            short_context = context[:, -batch_size:] # Reduce context to only focus on last batch_size of chars because positions are embedded\n","            predictions, loss = self(short_context)\n","            predictions = predictions[:, -1, :] \n","            probabilities = F.softmax(predictions, dim=-1) \n","            next_char = torch.multinomial(probabilities, num_samples=1)\n","            context = torch.cat((context, next_char), dim=1)\n","        return context\n","\n","model = BatchModel()\n","model = model.to(device) # Added ability to run on cuda\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) # Lowered the learning rate\n","\n","# Training loop\n","for i in range(10):\n","    for j in range(200):\n","        context, target = create_batches(train, batch_size=batch_size, batches=batches)\n","        predictions, loss = model(context, target)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","    print('train loss:', loss.item())\n","\n","    # Added test loss calculation to look for overfitting\n","    context, target = create_batches(test, batch_size=batch_size, batches=batches)\n","    predictions, loss = model(context, target)\n","    print('test loss:', loss.item())\n","\n","print('\\n',decoder(model.generate(context, length=300)[0][batch_size:].tolist()))"]},{"cell_type":"code","execution_count":15,"id":"5b3f8374","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:24:42.41993Z","iopub.status.busy":"2023-12-04T04:24:42.419634Z","iopub.status.idle":"2023-12-04T04:25:46.115487Z","shell.execute_reply":"2023-12-04T04:25:46.114524Z"},"papermill":{"duration":63.715488,"end_time":"2023-12-04T04:25:46.126313","exception":false,"start_time":"2023-12-04T04:24:42.410825","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["train loss: 1.5676820278167725\n","test loss: 1.6954126358032227\n","train loss: 1.611426591873169\n","test loss: 1.7233823537826538\n","train loss: 1.563042163848877\n","test loss: 1.6229959726333618\n","train loss: 1.5234103202819824\n","test loss: 1.4839695692062378\n","train loss: 1.4518576860427856\n","test loss: 1.6045912504196167\n","train loss: 1.490490198135376\n","test loss: 1.6036723852157593\n","train loss: 1.505974292755127\n","test loss: 1.739031195640564\n","train loss: 1.5470242500305176\n","test loss: 1.4754323959350586\n","train loss: 1.3111659288406372\n","test loss: 1.6211559772491455\n","train loss: 1.4039820432662964\n","test loss: 1.4038697481155396\n","\n"," g you home Like the miss at mess\n","Don't took it's gonna shoulde go you and tos ark tdarket, ever enever etting hold of that's hae sood\n","How no, so, what you were fightts\n","When you're call and opar\n","And the finfuch down you said, it with your falled on that you'll be a plaze is\n","Would you are me\n","Is it'dau\n"]}],"source":["# More training.\n","\n","for i in range(10):\n","    for j in range(200):\n","        context, target = create_batches(train, batch_size=batch_size, batches=batches)\n","        predictions, loss = model(context, target)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","    print('train loss:', loss.item())\n","\n","    context, target = create_batches(test, batch_size=batch_size, batches=batches)\n","    predictions, loss = model(context, target)\n","    print('test loss:', loss.item())\n","\n","print('\\n',decoder(model.generate(context, length=300)[0][batch_size:].tolist()))"]},{"cell_type":"code","execution_count":16,"id":"8352e7fb","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:25:46.144907Z","iopub.status.busy":"2023-12-04T04:25:46.144558Z","iopub.status.idle":"2023-12-04T04:26:49.642045Z","shell.execute_reply":"2023-12-04T04:26:49.641079Z"},"papermill":{"duration":63.518697,"end_time":"2023-12-04T04:26:49.653539","exception":false,"start_time":"2023-12-04T04:25:46.134842","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["train loss: 1.3756885528564453\n","test loss: 1.6416043043136597\n","train loss: 1.3626227378845215\n","test loss: 1.5092968940734863\n","train loss: 1.4083219766616821\n","test loss: 1.6576038599014282\n","train loss: 1.3332663774490356\n","test loss: 1.4309494495391846\n","train loss: 1.3838045597076416\n","test loss: 1.4327936172485352\n","train loss: 1.294456958770752\n","test loss: 1.4272412061691284\n","train loss: 1.3412097692489624\n","test loss: 1.5996131896972656\n","train loss: 1.3951127529144287\n","test loss: 1.5226457118988037\n","train loss: 1.3378328084945679\n","test loss: 1.7245795726776123\n","train loss: 1.19416081905365\n","test loss: 1.7106212377548218\n","\n"," sing\n","We was long in the drop ever\n","We go, it little little brough\n","And that you have pace out? Is is never\n","You and I call stul\n","And I could break alonely clush slow? so stay?\n","They'd love that what the ston?\n","I puct know it ever\n","We cannast let in the raid, road\n","And we sway a Momen Down\n","And you need capt \n"]}],"source":["# More training with reduced learning rate\n","learning_rate = 3e-4\n","\n","for i in range(10):\n","    for j in range(200):\n","        context, target = create_batches(train, batch_size=batch_size, batches=batches)\n","        predictions, loss = model(context, target)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","    print('train loss:', loss.item())\n","\n","    context, target = create_batches(test, batch_size=batch_size, batches=batches)\n","    predictions, loss = model(context, target)\n","    print('test loss:', loss.item())\n","\n","print('\\n',decoder(model.generate(context, length=300)[0][batch_size:].tolist()))"]},{"cell_type":"code","execution_count":17,"id":"b8e421c6","metadata":{"execution":{"iopub.execute_input":"2023-12-04T04:26:49.674404Z","iopub.status.busy":"2023-12-04T04:26:49.674119Z","iopub.status.idle":"2023-12-04T04:27:07.201821Z","shell.execute_reply":"2023-12-04T04:27:07.200888Z"},"papermill":{"duration":17.540852,"end_time":"2023-12-04T04:27:07.204115","exception":false,"start_time":"2023-12-04T04:26:49.663263","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," who love aloth\n","In preak it a thousong in a ghost\n","I hope of you, I want fadel in the ridice in the raching\n","Mll lords wise butting back too 've\n","Ralk just for so ask, this ever\n","Driving you made do\n","Plost of me forever\n","Iembody's don't never got bad knows\n","That we live gined to shat whoad Just know\n","If you play, still when it cuth ethis, passed and carvess\n","But I don't like you read of the name\n","You're tout too more but I come that I got hadd\n","What out, every think his town ve good a out\n","It's you wish at like 'oly dark of night anything, story\n","And two you that's help me\n","My bloving on ocons\n","Are, bey, now you stay home\n","But ston't know I'm gonet Boone\n","But I'm got overs\n","When then nothing throw in a were of warks\n","You don't wanna making me, but Honey, this londing down\n","In the cold guy on thing woods so down\n","Breaving at you one long time how that's gonna love\n","As betty thing ever leave\n","He was car bad it just come to you\n","My, I shadd out to be one end con the ploons you\n","The more to who oth one Isn't stile trying As the way I had to the skin\n","Lost times fears\n","Cross, thing is a longed to you\n","Before, fearlough\n","I'm doin't unywhere\n","To at that, less and I feelt caling\n","I feel hat it was sorrw\n","Lords gaves, shad me thing to had starling?\n","And I could never say I cronglow?\n","If your wads out, this cover stoon\n","There it's in the pattion padatiding\n","And I'm heard oe and you're do over gonna ever, dring\n","Don't factlon sky, and it's Meturt again\n","And in the bedrown now I've might\n","Like is why come girchan the mone\n","So m\n"]}],"source":["# Printing a longer generation output from the model\n","print('\\n',decoder(model.generate(context, length=1500)[1][batch_size:].tolist()))"]},{"cell_type":"markdown","id":"9ce5b5fc","metadata":{"papermill":{"duration":0.009731,"end_time":"2023-12-04T04:27:07.224075","exception":false,"start_time":"2023-12-04T04:27:07.214344","status":"completed"},"tags":[]},"source":["## Results\n","\n","Although the final output of the model sounds like gibberish, it greatly improved from its initial state where it vomited random characters. The model is able to produce real words, which is impressive given that it is only trained to predict the next character. The model has also learned to capitalize the first word of each line, and produces lines of lyrics that are on average the same length as the lyrics in the data set. I elected to stop training at this point because the model test error is only making small improvements with each iteration. \n","\n","This model shows the limitations of creating language models with a relatively small amount of compute power. Clearly, there is a lot of room for the model to improve. Using GPUs and distributed training would boost computational power and enable the model to become more complex with additional transformers, more embedding groups, and longer context sizes. Towards the end of the training, the model also started to suffer from overfitting, since the testing error was consistently higher than the training set error. To help reduce overfitting, I could introduce a dropping layer that would randomly drop some weights from the model; however, this would also significantly increase the training time to convergence. \n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1698936,"sourceId":2782773,"sourceType":"datasetVersion"}],"dockerImageVersionId":30527,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":231.839968,"end_time":"2023-12-04T04:27:09.588898","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-04T04:23:17.74893","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}