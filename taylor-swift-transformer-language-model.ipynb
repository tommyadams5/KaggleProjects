{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tommyadams/taylor-swift-transformer-language-model?scriptVersionId=141677320\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# <center> Taylor Swift Transformer Language Model </center>\n\n<center><img src='https://media.glamour.com/photos/59df542c017bd0228acd8003/4:3/w_2455,h_1841,c_limit/TAYLOR-SWIFT.jpg' height=600px width=600px>\n<img src='https://www.hearai.pl/post/13-slt/image5.png' height=600px width=600px></center>\n\n## Project Summary\n\nSince the publication of the 2017 paper \"Attention Is All You Need\", transformer language models have been shown to be very effective for a variety of natural language processing tasks and they have led to the creation of superior chat bots, such as Chat GPT. Transformer language models are a type of neural network that are used for natural language processing tasks such as machine translation, text summarization, and question answering. They are known for their ability to learn long-range dependencies between words, which makes them well-suited for tasks that require understanding the context of a sentence or paragraph. \n\nIn this project, I will build a transformer language model in PyTorch to generate lyrics in the style of Taylor Swift. I will do this by training the model on a dataset of lyrics from Taylor Swift's songs. The model will learn the patterns of Taylor Swift's writing style, and it will be able to use this knowledge to generate new lyrics that are similar to hers.\n\nThe project will be divided into the following steps:\n\n- Collect a dataset of lyrics from Taylor Swift's songs.\n- Prepare the dataset for training.\n- Build the transformer language model.\n- Train the model.\n- Evaluate the model's performance.\n- Generate new lyrics using the model.\n\n## Model Architecture\n\nMy model architecture will be based on the paper \"Attention Is All You Need\", which can be seen in the image above. However, there will be some differences between my model and the model in the paper:\n\n- My transformers will not have an encoder or cross-attention portions since I am not translating from one language to another like in the paper. As a result, my transformers will only perform masked self-attention.\n- I chose to do the normalization phase before the multi-head attention and feed forward portions of the model, as opposed to doing it afterwards like in the paper. I wanted to reduce gradients created by the attention and feed forward layers in the hopes that it would make training more stable. \n- The paper chose to use cosine and sine positional embeddings to teach the model to generalize for longer context lengths than it was trained on. I chose to use a simple ascending numbering of the positions since my model is much smaller and I do not expect comprehension of long context lengths to be a limiting factor.\n- Because I am limited by the computational power of my cpu, I chose to create a model with less layers, smaller embedding groups, and smaller training batch sizes. Also, I chose to have the model predict the next character as opposed to predicting the next word, which reduced the vocabulary of the model  and its computational intensity. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport glob","metadata":{"execution":{"iopub.status.busy":"2023-09-01T18:31:50.627647Z","iopub.execute_input":"2023-09-01T18:31:50.62804Z","iopub.status.idle":"2023-09-01T18:31:54.75424Z","shell.execute_reply.started":"2023-09-01T18:31:50.628011Z","shell.execute_reply":"2023-09-01T18:31:54.752915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the csv files and concatenating the data\n# Data set is from https://www.kaggle.com/datasets/thespacefreak/taylor-swift-song-lyrics-all-albums\npath = '/kaggle/input/taylor-swift-song-lyrics-all-albums/'\ncsv_files = glob.glob(path + \"/*.csv\")\ndf_list = (pd.read_csv(i) for i in csv_files)\ndf = pd.concat(df_list, ignore_index=True)\nlyrics = '\\n'.join(df.loc[:,'lyric']) ","metadata":{"execution":{"iopub.status.busy":"2023-09-01T18:33:43.593404Z","iopub.execute_input":"2023-09-01T18:33:43.594202Z","iopub.status.idle":"2023-09-01T18:33:43.710196Z","shell.execute_reply.started":"2023-09-01T18:33:43.594161Z","shell.execute_reply":"2023-09-01T18:33:43.70903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lyrics[:500])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-09-01T18:33:46.587114Z","iopub.execute_input":"2023-09-01T18:33:46.587528Z","iopub.status.idle":"2023-09-01T18:33:46.59412Z","shell.execute_reply.started":"2023-09-01T18:33:46.587497Z","shell.execute_reply":"2023-09-01T18:33:46.592979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of all unique characters\n' '.join(sorted(set(lyrics)))","metadata":{"execution":{"iopub.status.busy":"2023-08-18T19:24:46.512251Z","iopub.execute_input":"2023-08-18T19:24:46.512672Z","iopub.status.idle":"2023-08-18T19:24:46.526764Z","shell.execute_reply.started":"2023-08-18T19:24:46.512641Z","shell.execute_reply":"2023-08-18T19:24:46.525705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning the file by removing/replacing unnecessary characters and removing sections that are not lyrics\nreplace_with_space = ['\\u2005', '\\u200b', '\\u205f', '\\xa0', '-']\nreplace_letters = {'í':'i', 'é':'e', 'ï':'i', 'ó':'o', ';':',', '‘':'\\'', '’':'\\'', ':':',', 'е':'e'} \nremove_list = ['\\)', '\\(', '–','\"','”', '\"', '\\[.*\\]', '.*\\|.*', '—']\n\ncleaned_lyrics = lyrics\n\nfor old, new in replace_letters.items():\n    cleaned_lyrics = cleaned_lyrics.replace(old, new)\nfor string in remove_list:\n    cleaned_lyrics = re.sub(string,'',cleaned_lyrics)\nfor string in replace_with_space:\n    cleaned_lyrics = re.sub(string,' ',cleaned_lyrics)\nprint(''.join(sorted(set(cleaned_lyrics))))","metadata":{"execution":{"iopub.status.busy":"2023-08-18T19:25:39.893717Z","iopub.execute_input":"2023-08-18T19:25:39.894132Z","iopub.status.idle":"2023-08-18T19:25:39.94587Z","shell.execute_reply.started":"2023-08-18T19:25:39.894101Z","shell.execute_reply":"2023-08-18T19:25:39.94451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(lyrics), len(cleaned_lyrics))","metadata":{"execution":{"iopub.status.busy":"2023-08-18T19:25:43.985309Z","iopub.execute_input":"2023-08-18T19:25:43.985697Z","iopub.status.idle":"2023-08-18T19:25:43.991735Z","shell.execute_reply.started":"2023-08-18T19:25:43.985667Z","shell.execute_reply":"2023-08-18T19:25:43.990551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating an encoder and decoder to convert each character (char) to a number to feed into the model\nvocab = sorted(set(cleaned_lyrics))\nint_to_char = {int:char for int,char in enumerate(vocab)}\nchar_to_int = {char:int for int,char in enumerate(vocab)}\nencoder = lambda string: [char_to_int[char] for char in string] \ndecoder = lambda list: ''.join([int_to_char[i] for i in list]) \n\nprint(decoder(encoder(\"She's cheer captain\")))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting aside a portion for training the model and a portion for testing the data to prevent the model from overfitting to the data it is tested on\nlyric_tensor = torch.tensor(encoder(cleaned_lyrics), dtype=torch.long)\nsplit_point = int(len(lyric_tensor)*0.9)\ntrain = lyric_tensor[:split_point]\ntest = lyric_tensor[split_point:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a basic language model with only an embedding matrix. This model only references the most recent char to generate the next char\nclass BasicModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.char_embeddings = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, context):\n        predictions = self.char_embeddings(context)\n        return predictions\n\n    def generate(self, context, length):\n        for i in range(length):\n            predictions = self(context)\n            predictions = predictions[-1, :] # Only referencing most recent char\n            probabilities = F.softmax(predictions, dim=-1) # Normalize across the embedding dimension (aka vocab_size) so that they all add up to 1.00\n            next_char = torch.multinomial(probabilities, num_samples=1) # Samples randomly from the prob distribution of the embedding dimension\n            context = torch.cat((context, next_char))\n        return context\n\n# Selecting a random batch of text\ntorch.manual_seed(400)\nvocab_size = len(set(cleaned_lyrics))\nbatch_size = 20\nindex = torch.randint(low=0, high=len(train) - batch_size, size=(1,))\ncontext = train[index:(index+batch_size)]\n\n# Feeding the batch (context) into the model and asking it to generate text following it \nmodel = BasicModel(vocab_size)\npredictions = model(context)\nprint(' prediction dimensions:', predictions.shape) # Should have dimensions (batch_size, vocab_size)\nprint('\\n context input:', decoder(context.tolist())) # Context input\nprint('\\n context + response:', decoder( model.generate(context, length=30).tolist()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding ability for model to process multiple batches to improve training efficiency, and adding targets to measure loss\ndef create_batches(data, batch_size, batches):\n    index = torch.randint(low=0, high=len(data) - batch_size, size=(batches,))\n    context = torch.stack([data[row:(row+batch_size)] for row in index])\n    target = torch.stack([data[row+1:(row+batch_size+1)] for row in index]) # Target is just the context shifted one char to the right\n    return context, target\n\ncreate_batches(train, 10, 5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Expanding the basic model so that it can handle multiple batches at the same time. Also, added ability for model to calculate the loss function (cross_entropy).\nclass BatchModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.char_embeddings = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, context, target=None):\n        predictions = self.char_embeddings(context) # Context is the context output of create_batches and has dimensions (batches, batch_size)\n\n        if target == None:\n            loss = None\n        else:\n            # Resizing the shapes of predictions and target to meet requirements for cross_entropy loss function\n            A, B, C = predictions.shape\n            predictions = predictions.view(A * B, C)\n            target = target.view(A * B)\n            loss = F.cross_entropy(predictions, target)\n        \n        return predictions, loss\n\n    def generate(self, context, length):\n        for i in range(length):\n            predictions, loss = self(context)\n            predictions = predictions[:, -1, :] # Only referencing most recent char\n            probabilities = F.softmax(predictions, dim=-1) # Scale data across the embedding dimension of vocab_size so that they all add up to 1.00\n            next_char = torch.multinomial(probabilities, num_samples=1) # Samples randomly from the prob distribution of the embedding dimension\n            context = torch.cat((context, next_char), dim=1)\n        return context\n\nvocab_size = len(set(cleaned_lyrics))\ncontext, target = create_batches(train, batch_size=20, batches=2)\nmodel = BatchModel(vocab_size)\npredictions, loss = model(context, target)\noutput = model.generate(context, length=30)\nprint(' prediction dimensions:', predictions.shape) # Should have dimensions (batches * batch_size, vocab_size)\nprint('loss:', loss)\nprint('\\n batch1:\\n', decoder(output[0].tolist()))\nprint('\\n batch2:\\n', decoder(output[1].tolist()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using an optimizer to train the model. \noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\nfor i in range(10):\n    for i in range(100):\n        context, target = create_batches(train, batch_size=20, batches=30)\n        predictions, loss = model(context, target)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n    print(loss.item())\n\nprint('\\n context + response:\\n', decoder( model.generate(context, length=150)[0].tolist()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a head of attention so that the model can see past chars when predicting the next char\nclass Attention(nn.Module):\n\n    def __init__(self, batch_size, embed_groups, head_groups):\n        super().__init__()\n        self.query = nn.Linear(embed_groups, head_groups, bias=False) # Bias is set to false because a normalization layer follows\n        self.key = nn.Linear(embed_groups, head_groups, bias=False)\n        self.value = nn.Linear(embed_groups, head_groups, bias=False)\n        self.register_buffer('mask', torch.tril(torch.ones(batch_size, batch_size))) \n\n    def forward(self, x):\n        A,B,C = x.shape\n        query = self.query(x)\n        key = self.key(x)   \n\n        # This code results in dimensions of (A, B, B), which maps each char to each other char in the context. \n        # The matrix is multiplied by 1 / (embed_groups)**0.5 to prevent the soft max layer from sharpening to much in response to high dot product values\n        att = query @ key.transpose(-1,-2) * C**-0.5 \n\n        # Apply mask so that each char cannot 'see' future chars that come after it\n        att = att.masked_fill(self.mask[:B, :B] == 0, float('-inf')) \n\n        # Scale data to be between 0 and 1\n        att = F.softmax(att, dim=-1)\n        \n        value = self.value(x)\n        output = att @ value # Results in dimensions of (A, B, head_groups)\n        return output\n\nexample = Attention(30, 60, 20)\ninput = torch.rand(size=(15, 30, 60))\nprint(example(input).shape)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating multiple heads of attention, adding feed forward linear layers, stacking multiple transformers\n\n# Separated out the general parameters to make them easier to adjust\nvocab_size = len(set(cleaned_lyrics))\nbatches = 15 \nbatch_size = 30 \nembed_groups = 60\nnum_heads = 5\nhead_groups = embed_groups // num_heads\nlayers = 6  # Number of transformers\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef create_batches(data, batch_size, batches):\n    index = torch.randint(low=0, high=len(data) - batch_size, size=(batches,))\n    context = torch.stack([data[row:(row+batch_size)] for row in index])\n    target = torch.stack([data[row+1:(row+batch_size+1)] for row in index]) \n    context, target = context.to(device), target.to(device) # Added ability to run on cuda\n    return context, target\n\nclass Attention(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.query = nn.Linear(embed_groups, head_groups, bias=False)\n        self.key = nn.Linear(embed_groups, head_groups, bias=False)\n        self.value = nn.Linear(embed_groups, head_groups, bias=False)\n        self.register_buffer('mask', torch.tril(torch.ones(batch_size, batch_size)))\n\n    def forward(self, x):\n        A,B,C = x.shape\n        query = self.query(x)\n        key = self.key(x)   \n\n        att = query @ key.transpose(-1,-2) * C**-0.5 \n        att = att.masked_fill(self.mask[:B, :B] == 0, float('-inf')) \n        att = F.softmax(att, dim=-1)\n        \n        value = self.value(x)\n        output = att @ value\n        return output\n\nclass MultipleAttention(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.att_heads = nn.ModuleList([Attention() for i in range(num_heads)])\n        self.att_reader = nn.Linear(embed_groups, embed_groups)\n\n    def forward(self, x):\n        combined_att = torch.cat([i(x) for i in self.att_heads], dim=-1)\n        output = self.att_reader(combined_att)\n        return output\n\nclass FeedFoward(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.ff_network = nn.Sequential(\n            nn.Linear(embed_groups, 5 * embed_groups),\n            nn.ReLU(),\n            nn.Linear(5 * embed_groups, embed_groups))\n    # ReLU is added because it is a non-linear function, which allows the model to learn more complex relationships\n    \n    def forward(self, x):\n        return self.ff_network(x)\n\nclass Transformer(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.matt = MultipleAttention()\n        self.ff = FeedFoward()\n        self.linear1 = nn.LayerNorm(embed_groups)\n        self.linear2 = nn.LayerNorm(embed_groups)\n\n    def forward(self, x):\n        # Residuals are added to prevent vanishing gradient\n        x = x + self.matt(self.linear1(x)) \n        x = x + self.ff(self.linear2(x)) \n        return x\n\nclass BatchModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.char_embeddings = nn.Embedding(vocab_size, embed_groups)\n        self.pos_embeddings = nn.Embedding(batch_size, embed_groups) # Adds position embeddings to model can identify the order of the chars\n        self.transformers = nn.Sequential(*[Transformer() for i in range(layers)])\n        self.final_norm = nn.LayerNorm(embed_groups)\n        self.final_linear = nn.Linear(embed_groups, vocab_size)\n\n\n    def forward(self, context, target=None):\n        A, B = context.shape\n        full_embed = self.char_embeddings(context) + self.pos_embeddings(torch.arange(B, device=device))\n        x = self.transformers(full_embed)\n        x = self.final_norm(x)\n        predictions = self.final_linear(x)\n        \n        \n        if target == None:\n            loss = None\n        else:\n            A, B, C = predictions.shape\n            predictions = predictions.view(A * B, C)\n            target = target.view(A * B)\n            loss = F.cross_entropy(predictions, target)\n        \n        return predictions, loss\n\n    def generate(self, context, length):\n        for i in range(length):\n            short_context = context[:, -batch_size:] # Reduce context to only focus on last batch_size of chars because positions are embedded\n            predictions, loss = self(short_context)\n            predictions = predictions[:, -1, :] \n            probabilities = F.softmax(predictions, dim=-1) \n            next_char = torch.multinomial(probabilities, num_samples=1)\n            context = torch.cat((context, next_char), dim=1)\n        return context\n\nmodel = BatchModel()\nmodel = model.to(device) # Added ability to run on cuda\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) # Lowered the learning rate\n\n# Training loop\nfor i in range(10):\n    for j in range(200):\n        context, target = create_batches(train, batch_size=batch_size, batches=batches)\n        predictions, loss = model(context, target)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n    print('train loss:', loss.item())\n\n    # Added test loss calculation to look for overfitting\n    context, target = create_batches(test, batch_size=batch_size, batches=batches)\n    predictions, loss = model(context, target)\n    print('test loss:', loss.item())\n\nprint('\\n',decoder(model.generate(context, length=300)[0][batch_size:].tolist()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# More training.\n\nfor i in range(10):\n    for j in range(200):\n        context, target = create_batches(train, batch_size=batch_size, batches=batches)\n        predictions, loss = model(context, target)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n    print('train loss:', loss.item())\n\n    context, target = create_batches(test, batch_size=batch_size, batches=batches)\n    predictions, loss = model(context, target)\n    print('test loss:', loss.item())\n\nprint('\\n',decoder(model.generate(context, length=300)[0][batch_size:].tolist()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# More training with reduced learning rate\nlearning_rate = 3e-4\n\nfor i in range(10):\n    for j in range(200):\n        context, target = create_batches(train, batch_size=batch_size, batches=batches)\n        predictions, loss = model(context, target)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n    print('train loss:', loss.item())\n\n    context, target = create_batches(test, batch_size=batch_size, batches=batches)\n    predictions, loss = model(context, target)\n    print('test loss:', loss.item())\n\nprint('\\n',decoder(model.generate(context, length=300)[0][batch_size:].tolist()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing a longer generation output from the model\nprint('\\n',decoder(model.generate(context, length=1500)[1][batch_size:].tolist()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results\n\nAlthough the final output of the model sounds like gibberish, it greatly improved from its initial state where it vomited random characters. The model is able to produce real words, which is impressive given that it is only trained to predict the next character. The model has also learned to capitalize the first word of each line, and produces lines of lyrics that are on average the same length as the lyrics in the data set. I elected to stop training at this point because the model test error is only making small improvements with each iteration. \n\nThis model shows the limitations of creating language models with a relatively small amount of compute power. Clearly, there is a lot of room for the model to improve. Using GPUs and distributed training would boost computational power and enable the model to become more complex with additional transformers, more embedding groups, and longer context sizes. Towards the end of the training, the model also started to suffer from overfitting, since the testing error was consistently higher than the training set error. To help reduce overfitting, I could introduce a dropping layer that would randomly drop some weights from the model; however, this would also significantly increase the training time to convergence. \n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}