{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "55dfee90-3031-4bda-9373-6f09454b924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "<a href=\"https://www.kaggle.com/code/tommyadams/taylor-swift-transformer-language-model\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da503e-dc4e-466b-ac8f-90aaf8fcbafe",
   "metadata": {},
   "source": [
    "# <center> Taylor Swift Transformer Language Model </center>\n",
    "\n",
    "<center><img src='https://media.glamour.com/photos/59df542c017bd0228acd8003/4:3/w_2455,h_1841,c_limit/TAYLOR-SWIFT.jpg' height=600px width=600px>\n",
    "<img src='https://www.hearai.pl/post/13-slt/image5.png' height=600px width=600px></center>\n",
    "\n",
    "## Project Summary\n",
    "\n",
    "Since the publication of the 2017 paper \"Attention Is All You Need\", transformer language models have been shown to be very effective for a variety of natural language processing tasks and they have led to the creation of superior chat bots, such as Chat GPT. Transformer language models are a type of neural network that are used for natural language processing tasks such as machine translation, text summarization, and question answering. They are known for their ability to learn long-range dependencies between words, which makes them well-suited for tasks that require understanding the context of a sentence or paragraph. \n",
    "\n",
    "In this project, I will build a transformer language model in Pytorch to generate lyrics in the style of Taylor Swift. I will do this by training the model on a dataset of lyrics from Taylor Swift's songs. The model will learn the patterns of Taylor Swift's writing style, and it will be able to use this knowledge to generate new lyrics that are similar to hers.\n",
    "\n",
    "The project will be divided into the following steps:\n",
    "\n",
    "- Collect a dataset of lyrics from Taylor Swift's songs.\n",
    "- Prepare the dataset for training.\n",
    "- Build the transformer language model.\n",
    "- Train the model.\n",
    "- Evaluate the model's performance.\n",
    "- Generate new lyrics using the model.\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "My model architecture will be based on the paper \"Attention Is All You Need\", which can be seen in the image above. However, there will be some differences between my model and the model in the paper:\n",
    "\n",
    "- My transformers will not have an encoder or cross-attention portions since I am not translating from one language to another like in the paper. As a result, my transformers will only perform masked self-attention.\n",
    "- I chose to do the normalization phase before the multi-head attention and feed forward portions of the model, as opposed to doing it afterwards like in the paper. I wanted to reduce gradients created by the attention and feed forward layers in the hopes that it would make training more stable. \n",
    "- The paper chose to use cosine and sine positional embeddings to teach the model to generalize for longer context lengths than it was trained on. I chose to use a simple ascending numbering of the positions since my model is much smaller and I do not expect comprehension of long context lengths to be a limiting factor.\n",
    "- Because I am limited by the computational power of my cpu, I chose to create a model with less layers, smaller embedding groups, and smaller training batch sizes. Also, I chose to have the model predict the next character as opposed to predicting the next word, which reduced the vocabulary of the model  and its computational intensity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "752b80ea-969b-469c-bf15-08e537d2e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49611bca-9acc-459d-87f8-92cbc2451142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the csv files and concatenating the data\n",
    "# Data set is from https://www.kaggle.com/datasets/thespacefreak/taylor-swift-song-lyrics-all-albums\n",
    "path = 'path'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "df_list = (pd.read_csv(i) for i in csv_files)\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "lyrics = '\\n'.join(df.loc[:,'lyric']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f4456f-67d3-428f-8d69-147c9d5b7ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's something 'bout the way\n",
      "The street looks when it's just rained\n",
      "There's a glow off the pavement, you walk me to the car\n",
      "And you know I wanna ask you to dance right there\n",
      "In the middle of the parking lot, yeah\n",
      "Oh, yeah\n",
      "We're driving down the road, I wonder if you know\n",
      "I'm trying so hard not to get caught up now\n",
      "But you're just so cool, run your hands through your hair\n",
      "Absent-mindedly making me want you\n",
      "And I don't know how it gets better than this\n",
      "You take my hand and drag me head first, f\n"
     ]
    }
   ],
   "source": [
    "print(lyrics[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ecbfbc-d60c-42d5-b6ee-c77b031757ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   ! \" & \\' ( ) , - . 0 1 2 3 4 5 6 7 8 9 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y [ ] a b c d e f g h i j k l m n o p q r s t u v w x y z | \\xa0 é í ï ó е \\u2005 \\u200b – — ‘ ’ ” … \\u205f'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of all unique characters\n",
    "' '.join(sorted(set(lyrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70ec5ea-661d-4a16-b27d-b17ecc8d79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !',.0123456789?ABCDEFGHIJKLMNOPQRSTUVWXYabcdefghijklmnopqrstuvwxyz…\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the file by removing/replacing unnecessary characters and removing sections that are not lyrics\n",
    "replace_with_space = ['\\u2005', '\\u200b', '\\u205f', '\\xa0', '-']\n",
    "replace_letters = {'í':'i', 'é':'e', 'ï':'i', 'ó':'o', ';':',', '‘':'\\'', '’':'\\'', ':':',', 'е':'e'} \n",
    "remove_list = ['\\)', '\\(', '–','\"','”', '\"', '\\[.*\\]', '.*\\|.*', '—']\n",
    "\n",
    "cleaned_lyrics = lyrics\n",
    "\n",
    "for old, new in replace_letters.items():\n",
    "    cleaned_lyrics = cleaned_lyrics.replace(old, new)\n",
    "for string in remove_list:\n",
    "    cleaned_lyrics = re.sub(string,'',cleaned_lyrics)\n",
    "for string in replace_with_space:\n",
    "    cleaned_lyrics = re.sub(string,' ',cleaned_lyrics)\n",
    "print(''.join(sorted(set(cleaned_lyrics))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dee57f92-3ca7-4ea1-8044-d22b2f61c0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296805 293121\n"
     ]
    }
   ],
   "source": [
    "print(len(lyrics), len(cleaned_lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf169b0b-7348-43b3-b4c8-3d9f3756bf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She's cheer captain\n"
     ]
    }
   ],
   "source": [
    "# Creating an encoder and decoder to convert each character (char) to a number to feed into the model\n",
    "vocab = sorted(set(cleaned_lyrics))\n",
    "int_to_char = {int:char for int,char in enumerate(vocab)}\n",
    "char_to_int = {char:int for int,char in enumerate(vocab)}\n",
    "encoder = lambda string: [char_to_int[char] for char in string] \n",
    "decoder = lambda list: ''.join([int_to_char[i] for i in list]) \n",
    "\n",
    "print(decoder(encoder(\"She's cheer captain\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6dbd431-df7d-462f-91b7-7c9f76b332e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting aside a portion for training the model and a portion for testing the data to prevent the model from overfitting to the data it is tested on\n",
    "lyric_tensor = torch.tensor(encoder(cleaned_lyrics), dtype=torch.long)\n",
    "split_point = int(len(lyric_tensor)*0.9)\n",
    "train = lyric_tensor[:split_point]\n",
    "test = lyric_tensor[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d268ce04-6d9c-4622-8d22-27b2c333b7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " prediction dimensions: torch.Size([20, 69])\n",
      "\n",
      " context input: lose it all\n",
      "Losing h\n",
      "\n",
      " context + response: lose it all\n",
      "Losing h.JH7trIIIL\n",
      " pn?TL00l4T0HaMBML\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating a basic language model with only an embedding matrix. This model only references the most recent char to generate the next char\n",
    "class BasicModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        predictions = self.char_embeddings(context)\n",
    "        return predictions\n",
    "\n",
    "    def generate(self, context, length):\n",
    "        for i in range(length):\n",
    "            predictions = self(context)\n",
    "            predictions = predictions[-1, :] # Only referencing most recent char\n",
    "            probabilities = F.softmax(predictions, dim=-1) # Normalize across the embedding dimension (aka vocab_size) so that they all add up to 1.00\n",
    "            next_char = torch.multinomial(probabilities, num_samples=1) # Samples randomly from the prob distribution of the embedding dimension\n",
    "            context = torch.cat((context, next_char))\n",
    "        return context\n",
    "\n",
    "# Selecting a random batch of text\n",
    "torch.manual_seed(400)\n",
    "vocab_size = len(set(cleaned_lyrics))\n",
    "batch_size = 20\n",
    "index = torch.randint(low=0, high=len(train) - batch_size, size=(1,))\n",
    "context = train[index:(index+batch_size)]\n",
    "\n",
    "# Feeding the batch (context) into the model and asking it to generate text following it \n",
    "model = BasicModel(vocab_size)\n",
    "predictions = model(context)\n",
    "print(' prediction dimensions:', predictions.shape) # Should have dimensions (batch_size, vocab_size)\n",
    "print('\\n context input:', decoder(context.tolist())) # Context input\n",
    "print('\\n context + response:', decoder( model.generate(context, length=30).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38224801-9008-4f3b-8c2e-efab5d75c9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[45, 42, 66, 16,  0, 25,  1, 60, 42, 66],\n",
       "         [46,  4,  1, 66, 46, 42, 49,  0, 17, 55],\n",
       "         [ 1, 57, 46, 44, 62, 53, 50, 42, 59,  0],\n",
       "         [ 4,  1, 25,  3, 54,  1, 55, 46, 63, 46],\n",
       "         [ 1, 49, 50, 54,  0, 39, 56, 55, 45, 46]]),\n",
       " tensor([[42, 66, 16,  0, 25,  1, 60, 42, 66,  4],\n",
       "         [ 4,  1, 66, 46, 42, 49,  0, 17, 55, 45],\n",
       "         [57, 46, 44, 62, 53, 50, 42, 59,  0, 36],\n",
       "         [ 1, 25,  3, 54,  1, 55, 46, 63, 46, 59],\n",
       "         [49, 50, 54,  0, 39, 56, 55, 45, 46, 59]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding ability for model to process multiple batches to improve training efficiency, and adding targets to measure loss\n",
    "def create_batches(data, batch_size, batches):\n",
    "    index = torch.randint(low=0, high=len(data) - batch_size, size=(batches,))\n",
    "    context = torch.stack([data[row:(row+batch_size)] for row in index])\n",
    "    target = torch.stack([data[row+1:(row+batch_size+1)] for row in index]) # Target is just the context shifted one char to the right\n",
    "    return context, target\n",
    "\n",
    "create_batches(train, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5409011a-2dc2-4ea8-ae16-993e4448e5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " prediction dimensions: torch.Size([40, 69])\n",
      "loss: tensor(4.7122, grad_fn=<NllLossBackward0>)\n",
      "\n",
      " batch1:\n",
      " th Dali\n",
      "And they sairJL\n",
      "Un.YH7NO3f6iAy6aY?yREsE0Q4\n",
      "\n",
      " batch2:\n",
      " ke to love you\n",
      "Walk 6MS5z!oYkH j1z.5FuKnflpr5slfH \n"
     ]
    }
   ],
   "source": [
    "# Expanding the basic model so that it can handle multiple batches at the same time. Also, added ability for model to calculate the loss function (cross_entropy).\n",
    "class BatchModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, context, target=None):\n",
    "        predictions = self.char_embeddings(context) # Context is the context output of create_batches and has dimensions (batches, batch_size)\n",
    "\n",
    "        if target == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Resizing the shapes of predictions and target to meet requirements for cross_entropy loss function\n",
    "            A, B, C = predictions.shape\n",
    "            predictions = predictions.view(A * B, C)\n",
    "            target = target.view(A * B)\n",
    "            loss = F.cross_entropy(predictions, target)\n",
    "        \n",
    "        return predictions, loss\n",
    "\n",
    "    def generate(self, context, length):\n",
    "        for i in range(length):\n",
    "            predictions, loss = self(context)\n",
    "            predictions = predictions[:, -1, :] # Only referencing most recent char\n",
    "            probabilities = F.softmax(predictions, dim=-1) # Scale data across the embedding dimension of vocab_size so that they all add up to 1.00\n",
    "            next_char = torch.multinomial(probabilities, num_samples=1) # Samples randomly from the prob distribution of the embedding dimension\n",
    "            context = torch.cat((context, next_char), dim=1)\n",
    "        return context\n",
    "\n",
    "vocab_size = len(set(cleaned_lyrics))\n",
    "context, target = create_batches(train, batch_size=20, batches=2)\n",
    "model = BatchModel(vocab_size)\n",
    "predictions, loss = model(context, target)\n",
    "output = model.generate(context, length=30)\n",
    "print(' prediction dimensions:', predictions.shape) # Should have dimensions (batches * batch_size, vocab_size)\n",
    "print('loss:', loss)\n",
    "print('\\n batch1:\\n', decoder(output[0].tolist()))\n",
    "print('\\n batch2:\\n', decoder(output[1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8caf227-ca18-4efe-9554-0358d4f13023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6013240814208984\n",
      "2.9427413940429688\n",
      "2.6478054523468018\n",
      "2.5083088874816895\n",
      "2.444721221923828\n",
      "2.4227747917175293\n",
      "2.4300193786621094\n",
      "2.4064159393310547\n",
      "2.393315076828003\n",
      "2.3820302486419678\n",
      "\n",
      " context + response:\n",
      " n' to me\n",
      "Tell me whyonoqu mpind witld ck, au'me cl dawalit weVm8ryod h, rcesickn's ake hath meto5Kay I wolt widn apSou t'3d youromysoheadOowing ly wis whesh\n",
      "So I'th tit, \n"
     ]
    }
   ],
   "source": [
    "# Using an optimizer to train the model. \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "for i in range(10):\n",
    "    for i in range(100):\n",
    "        context, target = create_batches(train, batch_size=20, batches=30)\n",
    "        predictions, loss = model(context, target)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss.item())\n",
    "\n",
    "print('\\n context + response:\\n', decoder( model.generate(context, length=150)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f30bf1f-5db9-4049-8c0c-29f3074d5335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 30, 20])\n"
     ]
    }
   ],
   "source": [
    "# Creating a head of attention so that the model can see past chars when predicting the next char\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, embed_groups, head_groups):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_groups, head_groups, bias=False) # Bias is set to false because a normalization layer follows\n",
    "        self.key = nn.Linear(embed_groups, head_groups, bias=False)\n",
    "        self.value = nn.Linear(embed_groups, head_groups, bias=False)\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(batch_size, batch_size))) \n",
    "\n",
    "    def forward(self, x):\n",
    "        A,B,C = x.shape\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)   \n",
    "\n",
    "        # This code results in dimensions of (A, B, B), which maps each char to each other char in the context. \n",
    "        # The matrix is multiplied by 1 / (embed_groups)**0.5 to prevent the soft max layer from sharpening to much in response to high dot product values\n",
    "        att = query @ key.transpose(-1,-2) * C**-0.5 \n",
    "\n",
    "        # Apply mask so that each char cannot 'see' future chars that come after it\n",
    "        att = att.masked_fill(self.mask[:B, :B] == 0, float('-inf')) \n",
    "\n",
    "        # Scale data to be between 0 and 1\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        \n",
    "        value = self.value(x)\n",
    "        output = att @ value # Results in dimensions of (A, B, head_groups)\n",
    "        return output\n",
    "\n",
    "example = Attention(30, 60, 20)\n",
    "input = torch.rand(size=(15, 30, 60))\n",
    "print(example(input).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1104e7ad-5686-4a01-b798-beff62fc5f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.3442773818969727\n",
      "test loss: 2.219933032989502\n",
      "train loss: 2.166553497314453\n",
      "test loss: 2.097536325454712\n",
      "train loss: 1.9386600255966187\n",
      "test loss: 2.145120620727539\n",
      "train loss: 1.8186956644058228\n",
      "test loss: 1.9603970050811768\n",
      "train loss: 1.7928211688995361\n",
      "test loss: 1.7367510795593262\n",
      "train loss: 1.7265931367874146\n",
      "test loss: 1.798991322517395\n",
      "train loss: 1.6911594867706299\n",
      "test loss: 1.874962568283081\n",
      "train loss: 1.798145055770874\n",
      "test loss: 1.7075828313827515\n",
      "train loss: 1.716140627861023\n",
      "test loss: 1.5569429397583008\n",
      "train loss: 1.5284746885299683\n",
      "test loss: 1.4965180158615112\n",
      "\n",
      " Whoed you're ane on my\n",
      "Can't see the to the pinde tome\n",
      "And every, be who, e didm\n",
      "Soy nike on too you\n",
      "And me get etink mone me\n",
      "Lack my when, I'm linkes to door\n",
      "I know fore\n",
      "I wack you've be saining\n",
      "Like shoppy creeGp in this my sing cars\n",
      "And I, all you wish it feel it rrong on heah\n",
      "Mmiss when and thry\n"
     ]
    }
   ],
   "source": [
    "# Creating multiple heads of attention, adding feed forward linear layers, stacking multiple transformers\n",
    "\n",
    "# Separated out the general parameters to make them easier to adjust\n",
    "vocab_size = len(set(cleaned_lyrics))\n",
    "batches = 15 \n",
    "batch_size = 30 \n",
    "embed_groups = 60\n",
    "num_heads = 5\n",
    "head_groups = embed_groups // num_heads\n",
    "layers = 6  # Number of transformers\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def create_batches(data, batch_size, batches):\n",
    "    index = torch.randint(low=0, high=len(data) - batch_size, size=(batches,))\n",
    "    context = torch.stack([data[row:(row+batch_size)] for row in index])\n",
    "    target = torch.stack([data[row+1:(row+batch_size+1)] for row in index]) \n",
    "    context, target = context.to(device), target.to(device) # Added ability to run on cuda\n",
    "    return context, target\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_groups, head_groups, bias=False)\n",
    "        self.key = nn.Linear(embed_groups, head_groups, bias=False)\n",
    "        self.value = nn.Linear(embed_groups, head_groups, bias=False)\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(batch_size, batch_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        A,B,C = x.shape\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)   \n",
    "\n",
    "        att = query @ key.transpose(-1,-2) * C**-0.5 \n",
    "        att = att.masked_fill(self.mask[:B, :B] == 0, float('-inf')) \n",
    "        att = F.softmax(att, dim=-1)\n",
    "        \n",
    "        value = self.value(x)\n",
    "        output = att @ value\n",
    "        return output\n",
    "\n",
    "class MultipleAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att_heads = nn.ModuleList([Attention() for i in range(num_heads)])\n",
    "        self.att_reader = nn.Linear(embed_groups, embed_groups)\n",
    "\n",
    "    def forward(self, x):\n",
    "        combined_att = torch.cat([i(x) for i in self.att_heads], dim=-1)\n",
    "        output = self.att_reader(combined_att)\n",
    "        return output\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ff_network = nn.Sequential(\n",
    "            nn.Linear(embed_groups, 5 * embed_groups),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5 * embed_groups, embed_groups))\n",
    "    # ReLU is added because it is a non-linear function, which allows the model to learn more complex relationships\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.ff_network(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.matt = MultipleAttention()\n",
    "        self.ff = FeedFoward()\n",
    "        self.linear1 = nn.LayerNorm(embed_groups)\n",
    "        self.linear2 = nn.LayerNorm(embed_groups)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residuals are added to prevent vanishing gradient\n",
    "        x = x + self.matt(self.linear1(x)) \n",
    "        x = x + self.ff(self.linear2(x)) \n",
    "        return x\n",
    "\n",
    "class BatchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embed_groups)\n",
    "        self.pos_embeddings = nn.Embedding(batch_size, embed_groups) # Adds position embeddings to model can identify the order of the chars\n",
    "        self.transformers = nn.Sequential(*[Transformer() for i in range(layers)])\n",
    "        self.final_norm = nn.LayerNorm(embed_groups)\n",
    "        self.final_linear = nn.Linear(embed_groups, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, context, target=None):\n",
    "        A, B = context.shape\n",
    "        full_embed = self.char_embeddings(context) + self.pos_embeddings(torch.arange(B, device=device))\n",
    "        x = self.transformers(full_embed)\n",
    "        x = self.final_norm(x)\n",
    "        predictions = self.final_linear(x)\n",
    "        \n",
    "        \n",
    "        if target == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            A, B, C = predictions.shape\n",
    "            predictions = predictions.view(A * B, C)\n",
    "            target = target.view(A * B)\n",
    "            loss = F.cross_entropy(predictions, target)\n",
    "        \n",
    "        return predictions, loss\n",
    "\n",
    "    def generate(self, context, length):\n",
    "        for i in range(length):\n",
    "            short_context = context[:, -batch_size:] # Reduce context to only focus on last batch_size of chars because positions are embedded\n",
    "            predictions, loss = self(short_context)\n",
    "            predictions = predictions[:, -1, :] \n",
    "            probabilities = F.softmax(predictions, dim=-1) \n",
    "            next_char = torch.multinomial(probabilities, num_samples=1)\n",
    "            context = torch.cat((context, next_char), dim=1)\n",
    "        return context\n",
    "\n",
    "model = BatchModel()\n",
    "model = model.to(device) # Added ability to run on cuda\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) # Lowered the learning rate\n",
    "\n",
    "# Training loop\n",
    "for i in range(10):\n",
    "    for j in range(200):\n",
    "        context, target = create_batches(train, batch_size=batch_size, batches=batches)\n",
    "        predictions, loss = model(context, target)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('train loss:', loss.item())\n",
    "\n",
    "    # Added test loss calculation to look for overfitting\n",
    "    context, target = create_batches(test, batch_size=batch_size, batches=batches)\n",
    "    predictions, loss = model(context, target)\n",
    "    print('test loss:', loss.item())\n",
    "\n",
    "print('\\n',decoder(model.generate(context, length=300)[0][batch_size:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1028e055-9ed4-4581-a3b0-b49d334a242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.55916166305542\n",
      "test loss: 1.6342307329177856\n",
      "train loss: 1.5800981521606445\n",
      "test loss: 1.505556344985962\n",
      "train loss: 1.5744961500167847\n",
      "test loss: 1.522669792175293\n",
      "train loss: 1.6049600839614868\n",
      "test loss: 1.4857109785079956\n",
      "train loss: 1.4192758798599243\n",
      "test loss: 1.5816426277160645\n",
      "train loss: 1.537201166152954\n",
      "test loss: 1.5125099420547485\n",
      "train loss: 1.7224860191345215\n",
      "test loss: 1.3999484777450562\n",
      "train loss: 1.3095953464508057\n",
      "test loss: 1.5539305210113525\n",
      "train loss: 1.540711522102356\n",
      "test loss: 1.3750193119049072\n",
      "train loss: 1.2430648803710938\n",
      "test loss: 1.405914068222046\n",
      "\n",
      " age?\n",
      "We be here, never me to the land everythe wile\n",
      "This is a grown\n",
      "The ring everything a bouse\n",
      "I wish mine spooty\n",
      "You know me, hope of you our to be must a farty to conean still at they3Skeepy you tolcry\n",
      "I know that you head break there\n",
      "In red and your said go\n",
      "This with through first walk together\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# More training.\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(200):\n",
    "        context, target = create_batches(train, batch_size=batch_size, batches=batches)\n",
    "        predictions, loss = model(context, target)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('train loss:', loss.item())\n",
    "\n",
    "    context, target = create_batches(test, batch_size=batch_size, batches=batches)\n",
    "    predictions, loss = model(context, target)\n",
    "    print('test loss:', loss.item())\n",
    "\n",
    "print('\\n',decoder(model.generate(context, length=300)[0][batch_size:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca047161-0c31-4821-9528-9f65175c37c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.5085272789001465\n",
      "test loss: 1.6270062923431396\n",
      "train loss: 1.4490337371826172\n",
      "test loss: 1.3476649522781372\n",
      "train loss: 1.458145022392273\n",
      "test loss: 1.5336698293685913\n",
      "train loss: 1.386934518814087\n",
      "test loss: 1.6454744338989258\n",
      "train loss: 1.2708995342254639\n",
      "test loss: 1.5291829109191895\n",
      "train loss: 1.3878862857818604\n",
      "test loss: 1.53359854221344\n",
      "train loss: 1.2063854932785034\n",
      "test loss: 1.708042860031128\n",
      "train loss: 1.4834686517715454\n",
      "test loss: 1.5367110967636108\n",
      "train loss: 1.2053431272506714\n",
      "test loss: 1.3365861177444458\n",
      "train loss: 1.2643754482269287\n",
      "test loss: 1.5570825338363647\n",
      "\n",
      " spaintly\n",
      "Honey, some it all and facGe\n",
      "You body or the se gouna flyone\n",
      "You know Yist do\n",
      "Everyone\n",
      "I don't wanna say you would AM I will Never I'T just sor that I'm alone\n",
      "I saidn't never and I'd bried\n",
      "I did rui losss Bug kepping rit it all\n",
      "I bright sight that your haold\n",
      "You're calming if you could down\n"
     ]
    }
   ],
   "source": [
    "# More training with reduced learning rate\n",
    "learning_rate = 3e-4\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(200):\n",
    "        context, target = create_batches(train, batch_size=batch_size, batches=batches)\n",
    "        predictions, loss = model(context, target)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('train loss:', loss.item())\n",
    "\n",
    "    context, target = create_batches(test, batch_size=batch_size, batches=batches)\n",
    "    predictions, loss = model(context, target)\n",
    "    print('test loss:', loss.item())\n",
    "\n",
    "print('\\n',decoder(model.generate(context, length=300)[0][batch_size:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f94af740-b05e-4c40-a529-67c42d88a042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " nge you when you're shine\n",
      "And I just faked stray\n",
      "Some it placess blue\n",
      "I'll just spick up, baby\n",
      "I'm walk you here?\n",
      "I'd never been wildoods a big simpone\n",
      "I wouldn't just like the she gaws you like my down\n",
      "Treamicantious\n",
      "I never lugst so you were ight shrought is the othern Wide sleave\n",
      "You were right there dress breakinets Yeah\n",
      "Trook me this, time, I long out Oh\n",
      "eyed Long out of the Window\n",
      "My shine just get anone, shall me, bab, blash in my higure\n",
      "I'm on my pressisterst\n",
      "'Cause I'm beanging for this, time\n",
      "No do o, on I really my like stay\n",
      "Carming you all the cool the nights, they fight just get his that you inless to me\n",
      "Think a sing I hope\n",
      "She's every give my teach\n",
      "And now I think ho, head pinning\n",
      "But I know I chon down\n",
      "Why would you just took think I sick and and\n",
      "This walling was dang of you\n",
      "One the us lovers\n",
      "This is the sames, sing all as why\n",
      "\n",
      "The lot you're in again braking they safritist\n",
      "Don't said me can eme as gorebous up oor blew on the way windows\n",
      "He's so high I feell down and tiradic of I broked up up and I Ind need that it's a littles I don't we know you, his Con, my mmpace\n",
      "There's Spay in the dim me\n",
      "Ml\n",
      "If I this cratifs, I know where you You want to\n",
      "'Til the sips, Wishinr I did you\n",
      "Walk the weyode thing ecome\n",
      "I could see things offside you shope, keames\n",
      "I might carletow Mmistin' on all you the way for Mr. I hope I still real in the way home, was calling?\n",
      "I'm along your face, love lost with you\n",
      "Wela I wide\n",
      "But we encyember in the rain the last bedsheets it like I'm foll\n"
     ]
    }
   ],
   "source": [
    "# Printing a longer generation output from the model\n",
    "print('\\n',decoder(model.generate(context, length=1500)[1][batch_size:].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f50ac2-7a66-499e-abbf-824cf5bd8849",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Although the final output of the model sounds like gibberish, it greatly improved from its initial state where it vomited random characters. The model is able to produce real words, which is impressive given that it is only trained to predict the next character. The model has also learned to capitalize the first word of each line, and produces lines of lyrics that are on average the same length as the lyrics in the data set. I elected to stop training at this point because the model test error is only making small improvements with each iteration. \n",
    "\n",
    "This model shows the limitations of creating language models with a relatively small amount of compute power. Clearly, there is a lot of room for the model to improve. Using a gpus and distributed training would boost computational power and enable the model to become more complex with additional transformers, more embedding groups, and longer context sizes. Towards the end of the training, the model also started to suffer from overfitting, since the testing error was consistently higher than the training set error. To help reduce overfitting, I could introduce a dropping layer that would randomly drop some weights from the model; however, this would also significantly increase the training time to convergence. \n",
    "\n",
    "In conclusion, here are some of the more humorous lines from the final output:\n",
    "\n",
    "\"I could see things offside you\"\n",
    "\n",
    "\"I hope I still real in the way home\"\n",
    "\n",
    "\"I'm along your face, love lost with you\"\n",
    "\n",
    "\"I'm walk you here?\"\n",
    "\n",
    "\"This walling was dang of you\"\n",
    "\n",
    "\"And I just faked stray\"\n",
    "\n",
    "\"One the us lovers\"\n",
    "\n",
    "\"This is the sames, sing all as why\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
