{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center>RAG Vector Search with 20,000 Leagues Under the Sea</center>\n<center><img src='https://static.independent.co.uk/s3fs-public/thumbnails/image/2015/11/10/09/johnny-depp.jpg' width=800></center>","metadata":{}},{"cell_type":"markdown","source":"### Introduction\n\nLarge language models (LLMs) are powerful tools that can generate text, translate languages, and answer questions in an informative way. However, LLMs often lack specific knowledge about certain domains. Retrieval-Augmented Generation (RAG) is a prompt-engineering technique that can improve the specific knowledge of LLMs by providing them with relevant context from a database. When a user prompts the LLM, RAG searches a database for relevant context based on the prompt, and then injects the context into the prompt before feeding the prompt into the LLM. The LLM is then able to provide a more informed answer based on the context. RAG has numerous applications, particularly for businesses seeking to easily search their extensive private databases for domain-specific knowledge.\n\n### Project Summary\n\nIn this project, I used RAG to improve an LLM's specific knowledge of 20,000 Leagues Under the Sea (Leagues), a classic novel written by Jules Verne. In addition, I employed prompt engineering techniques to have the LLM talk in a style similar to Jack Sparrow from Pirates of the Carribbean. \n\nI used Chat GPT 3.5 as the LLM, Langchain to segment the text into chunks, and Pinecone to create a vector database. A vector database is a NoSQL database that stores chunks of information as numerical vectors, where the distance between each vector correlates to the similarity between those vectors. In this project, I utilized Open AI's embedding model to map each text chunk from Leagues to a vector before uploading it to the Pinecone database. When querying the vector database, the query is converted into a vector and then the database returns the vectors that are located closest to the query vector. Those vectors are then converted back into text before being sent to the user. \n\n### Results\n\nAs shown below, the LLM was able to learn specific knowledge about Leagues and correctly answer questions such as \"While aboard the Monroe, how many whales did Ned Land kill?\" RAG queries outperformed traditional queries; however, it depended on question phrasing and the vector database search function appeared to be the weak link in the process. \n\nThe search function often struggled to understand the semantic meaning of queries. Queries failed when query terms didn't precisely match the text, and queries containing substantial text not present in the vector database were problematic. One potential solution could involve initial query interpretation by an LLM to identify relevant keywords and synonyms not explicitly mentioned in the query.\n\nSometimes, the database query did not provide enough context for the LLM to fully answer the prompt. To fix this issue, I initially experimented with using larger text chunks, but this impaired the search function's ability to find the correct matches. This is probably because larger chunks dilute the strength of certain keyword matches between vectors. I later solved this issue by keeping chunk sizes small, and instructing the search function to retrieve adjacent text chunks for the matching chunks.\n\nI had some success with getting the model to sound like Jack Sparrow, but it seemed to be lacking some of Jack's quirky, enigmatic, and humorous traits. Improvements could be made by enhancing the quality and quantity of Jack Sparrow dialogue examples or conducting a separate RAG on a Jack Sparrow dialogue database. But my guess is that this is more a limitation with Chat GPT 3.5. I am used to Jack Sparrow always sounding witty and humorous because his movie dialogue was meticulously crafted to delight audiences. It would be expecting a lot for a language model to replicate this high level of humor and wit when answering every question. \n\nOverall, this project demonstrated the ability of RAG to augment an LLM's domain specific knowledge while shedding light on the limitations of vector similarity searches. ","metadata":{}},{"cell_type":"code","source":"import os\nimport pinecone\nimport re\n\nfrom tqdm.auto import tqdm\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.vectorstores import Pinecone\nfrom langchain.schema import (AIMessage,SystemMessage,HumanMessage)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing text file for 20k Leagues\nwith open('20kLeagues.txt', 'r') as file:\n    data = file.read()\n\nprint(data[:1000])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing vector database\npinecone.init(api_key='YOUR KEY HERE', environment='gcp-starter')\npinecone.create_index('ragsea', dimension=1536, metric='cosine')\nindex = pinecone.Index('ragsea')\nindex.describe_index_stats()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accessing OpenAI API\nos.environ['OPENAI_API_KEY'] ='YOUR KEY HERE'\nchat = ChatOpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"], model='gpt-3.5-turbo')\nembed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting 20k Leagues text into chunks and loading them into the vector database\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 256,\n    chunk_overlap  = 0,\n    length_function = len,\n    is_separator_regex = False,\n)\n\ntexts = text_splitter.split_text(data)\nids = [str(x) for x in range(len(texts))]\nembeds = embed_model.embed_documents(texts)\nmetadata = [ {'text': x} for x in texts]\nbatch_size = 100\n\nfor i in tqdm(range(0, len(texts), batch_size)): \n    i_end = min(i+batch_size, len(texts))\n    index.upsert(vectors=zip(ids[i:i_end], embeds[i:i_end], metadata[i:i_end]))\n\nindex.describe_index_stats()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Querying database\nvectorstore = Pinecone(index, embed_model.embed_query, \"text\")\nvectorstore.similarity_search(\"What happened on the 20th of July, 1866?\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combining context from vector database with the prompt function\ndef augment_prompt(query):\n    results = vectorstore.similarity_search(query)\n    source_knowledge = \"\\n\".join([x.page_content for x in results])\n    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n\n    Contexts:\n    {source_knowledge}\n\n    Query: {query}\"\"\"\n    return augmented_prompt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating class to combine queries into a conversation\nclass Discussion:\n    def __init__ (self, logs=None):\n        if logs is None:\n            logs = []\n        self.logs = logs \n        \n    def ask(self, query):\n        self.logs.append(HumanMessage(content=query))\n        response = chat(self.logs)\n        self.logs.append(response)\n        print(response.content)\n\n    def ask_rag(self, query):\n        self.logs.append(HumanMessage(content=augment_prompt(query)))\n        response = chat(self.logs)\n        self.logs.append(response)\n        print(response.content)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing base prompt on 20k Leagues question. \ndiscussion1 = Discussion()\ndiscussion1.ask(\"What happened on the 20th of July, 1866?\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing RAG prompt with context from vector database\ndiscussion1 = Discussion()\ndiscussion1.ask_rag(\"What happened on the 20th of July, 1866?\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Expanding context sizes to provide information on chunks that precede and follow the matching chunks\n\nembed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 256,\n    chunk_overlap  = 0, \n    length_function = len,\n    is_separator_regex = False,\n)\n\ntexts = text_splitter.split_text(data)\n\n# Adding character counts indices to end of each chunk in order to easily index the neighboring chunks\nchar_idx = [len(texts[0])-1]\nfor i in range(1,len(texts)):\n    char_idx.append(char_idx[-1] + len(texts[i]))\n\ntexts_ids = [f'<id={char_idx[x]}>' + texts[x] for x in range(len(texts))]\n\nids = [str(x) for x in range(len(texts))]\nembeds = embed_model.embed_documents(texts)\nmetadata = [ {'text': x} for x in texts_ids]\nbatch_size = 100\n\nfor i in tqdm(range(0, len(texts), batch_size)): \n    i_end = min(i+batch_size, len(texts))\n    index.upsert(vectors=zip(ids[i:i_end], embeds[i:i_end], metadata[i:i_end]))\n\nindex.describe_index_stats()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Expanded source knowledge to include the preceding chunk and the 4 following chunks for each match\ndef augment_prompt(query):\n    results = vectorstore.similarity_search(query)\n    results = ''.join([x.page_content for x in results])\n    char_idxs = re.findall(r'<id=([0-9]*)>', results)\n    source_knowledge = \"\\n\".join([data[max(int(x)-256*2,0):min(len(data),int(x)+256*4)] for x in char_idxs])\n    augmented_prompt = f\"\"\"Using the contexts below, answer the query using 1000 characters or less.\n\n    Contexts:\n    {source_knowledge}\n\n    Query: {query}\"\"\"\n    return augmented_prompt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing RAG prompt with expanded context\ndiscussion1 = Discussion()\ndiscussion1.ask_rag(\"What happened on the 20th of July, 1866?\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('JackSparrow.txt', 'r') as file:\n    jack_sparrow = file.read()\n\nquestion = \"Describe the tone, style, language, and sentence length of Captain Jack Sparrow in the following text: \"\ndiscussion1 = Discussion()\njack_style = discussion1.ask(question + jack_sparrow) \njack_style","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a prompt for Chat GPT to imitate the style of Jack Sparrow\n\ncharacter_prompt = \"\"\"You are Captain Jack Sparrow, the enigmatic pirate captain of the Black Pearl. \nAnswer all queries in his style of taking based on this description: \"\"\" + str(jack_style)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discussion1 = Discussion([SystemMessage(content=character_prompt)])\ndiscussion1.ask_rag(\"Tell me about yourself\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing RAG prompt\ndiscussion1 = Discussion([SystemMessage(content=character_prompt)])\ndiscussion1.ask_rag(\"What happened on the 20th of July, 1866?\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing RAG prompt\ndiscussion1 = Discussion([SystemMessage(content=character_prompt)])\ndiscussion1.ask_rag('While aboard the Monroe, how many whales did Ned Land kill?')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing base prompt\ndiscussion1 = Discussion([SystemMessage(content=character_prompt)])\ndiscussion1.ask('While aboard the Monroe, how many whales did Ned Land kill?')","metadata":{},"execution_count":null,"outputs":[]}]}